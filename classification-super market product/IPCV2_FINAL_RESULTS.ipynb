{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook just contains the final results, the background works which help to conclude that this structures can have these accuracies are in other Notebooks."
      ],
      "metadata": {
        "id": "6lFvsCjEbPP0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "for PART1 we used dense-net like structure which learned on image-net (as transfer learning) and trained 3 different individual to train a ensemble model.\n",
        "in PART2 however, we just used mixcut augmentation, criterion, and some other regularization methods.\n",
        "as we examined in other Notebooks, with method we used here in part1, we got lower accuracy in part2.\n",
        "also with method we used here in part2, we got lower accuracy if use it in part1 too."
      ],
      "metadata": {
        "id": "V7exMYA_cbhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PART1"
      ],
      "metadata": {
        "id": "cGVDeIkhZmgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super(DenseLayer, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(self.relu(self.bn1(x)))\n",
        "        out = torch.cat([x, out], 1)\n",
        "        return out\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, n_layers):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(n_layers):\n",
        "            layers.append(DenseLayer(in_channels + i * growth_rate, growth_rate))\n",
        "        self.layer = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class TransitionLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(TransitionLayer, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(self.relu(self.bn(x)))\n",
        "        out = self.pool(out)\n",
        "        return out\n",
        "\n",
        "class CustomDenseNet(nn.Module):\n",
        "    def __init__(self, num_classes=43, growth_rate=32, block_layers=[6, 12, 24, 16]):\n",
        "        super(CustomDenseNet, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "        num_blocks = len(block_layers)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Dense blocks and transition layers\n",
        "        in_channels = 64\n",
        "        self.dense_blocks = nn.ModuleList()\n",
        "        self.trans_layers = nn.ModuleList()\n",
        "        for i in range(num_blocks):\n",
        "            self.dense_blocks.append(DenseBlock(in_channels, growth_rate, block_layers[i]))\n",
        "            in_channels += growth_rate * block_layers[i]\n",
        "            if i != num_blocks - 1:  # No transition layer after the last dense block\n",
        "                out_channels = in_channels // 2\n",
        "                self.trans_layers.append(TransitionLayer(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "        self.fc = nn.Linear(in_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
        "        for i in range(len(self.dense_blocks)):\n",
        "            x = self.dense_blocks[i](x)\n",
        "            if i != len(self.dense_blocks) - 1:\n",
        "                x = self.trans_layers[i](x)\n",
        "        x = self.relu(self.bn2(x))\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=15, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.best_model_wts = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decreases.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        self.best_model_wts = model.state_dict()\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "    def get_best_model_wts(self):\n",
        "        return self.best_model_wts\n",
        "\n",
        "def train_model_customCNN(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=25, patience=15):\n",
        "    best_val_acc = 0.0\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = val_correct / val_total\n",
        "        val_losses.append(val_epoch_loss)\n",
        "        val_accuracies.append(val_epoch_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
        "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "        early_stopping(val_epoch_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            model.load_state_dict(early_stopping.get_best_model_wts())\n",
        "            break\n",
        "\n",
        "        if val_epoch_acc > best_val_acc:\n",
        "            best_val_acc = val_epoch_acc\n",
        "            best_model_wts = model.state_dict()\n",
        "\n",
        "        if isinstance(scheduler, ReduceLROnPlateau):\n",
        "            scheduler.step(val_epoch_loss)  # Pass validation loss to scheduler\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    misclassifications = defaultdict(list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Collect misclassified examples\n",
        "            for label, prediction in zip(labels, predicted):\n",
        "                if label != prediction:\n",
        "                    misclassifications[label.item()].append(prediction.item())\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f'Test Accuracy: {acc:.4f}')\n",
        "\n",
        "    # Print misclassification details\n",
        "    for label, predictions in misclassifications.items():\n",
        "        print(f'{len(predictions)} data from class number {label} wrongly labeled as {predictions}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T17:40:55.582604Z",
          "iopub.execute_input": "2024-07-01T17:40:55.582946Z",
          "iopub.status.idle": "2024-07-01T17:40:55.622723Z",
          "shell.execute_reply.started": "2024-07-01T17:40:55.582921Z",
          "shell.execute_reply": "2024-07-01T17:40:55.621802Z"
        },
        "trusted": true,
        "id": "-TiuU42uTfOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training function\n",
        "def train_model_imagenet(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=20):\n",
        "    best_val_acc = 0.0\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    early_stopping = EarlyStopping(patience=15)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = val_correct / val_total\n",
        "        val_losses.append(val_epoch_loss)\n",
        "        val_accuracies.append(val_epoch_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
        "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_epoch_acc)\n",
        "\n",
        "        if val_epoch_acc > best_val_acc:\n",
        "            best_val_acc = val_epoch_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            torch.save(optimizer.state_dict(), 'best_opt.pth')\n",
        "\n",
        "    print(f'Best Validation Accuracy: {best_val_acc:.4f}')\n",
        "    return model, train_losses, val_losses, train_accuracies, val_accuracies\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T15:12:31.171404Z",
          "iopub.execute_input": "2024-07-02T15:12:31.172102Z",
          "iopub.status.idle": "2024-07-02T15:12:31.185777Z",
          "shell.execute_reply.started": "2024-07-02T15:12:31.172074Z",
          "shell.execute_reply": "2024-07-02T15:12:31.184839Z"
        },
        "trusted": true,
        "id": "X5Qzn5D0TfOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####LEARN ON IMAGENET"
      ],
      "metadata": {
        "id": "I-Fj-jpoam76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "model_copy = CustomDenseNet(1000)  # Replace with model class\n",
        "\n",
        "# Load the state_dict into the new model instance\n",
        "model_copy.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "# Load the optimizer state_dict\n",
        "#checkpoint = torch.load('/kaggle/working/best_model.pth')\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "model_copy.to(device)\n",
        "new_model, train_losses, val_losses, train_accuracies, val_accuracies = train_model_imagenet(model_copy, nn.CrossEntropyLoss(), optimizer, scheduler, test_loader_imgnt, val_loader_imgnt, num_epochs=epochs)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T15:12:40.257916Z",
          "iopub.execute_input": "2024-07-02T15:12:40.258786Z",
          "iopub.status.idle": "2024-07-02T17:37:51.321875Z",
          "shell.execute_reply.started": "2024-07-02T15:12:40.258748Z",
          "shell.execute_reply": "2024-07-02T17:37:51.320258Z"
        },
        "trusted": true,
        "id": "PlJojmDETfOJ",
        "outputId": "b65f46b2-248e-4a4f-c73c-3e8281c1b7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/40] Train Loss: 4.3267, Train Acc: 0.2360, Val Loss: 4.1243, Val Acc: 0.2679\nEpoch [2/40] Train Loss: 4.3267, Train Acc: 0.2360, Val Loss: 4.1243, Val Acc: 0.2679\nEpoch [3/40] Train Loss: 4.3267, Train Acc: 0.2360, Val Loss: 4.1243, Val Acc: 0.2679\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the optimizer state_dict\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#checkpoint = torch.load('/kaggle/working/best_model.pth')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model_copy\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m new_model, train_losses, val_losses, train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_imagenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader_imgnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader_imgnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mtrain_model_imagenet\u001b[0;34m(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "num_classes = 43\n",
        "\n",
        "class GroceryStoreDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[str, int]] = None, split: str = None, transform=None) -> None:\n",
        "        super().__init__()\n",
        "        self.root = Path(\"/kaggle/working/GroceryStoreDataset/dataset\")\n",
        "        self.transform = transform\n",
        "\n",
        "        if samples is not None:\n",
        "            self.samples = samples\n",
        "            self.labels = [label for _, label in samples]\n",
        "            self.paths = [path for path, _ in samples]\n",
        "        else:\n",
        "            self.split = split\n",
        "            self.paths, self.labels = self.read_file()\n",
        "\n",
        "        self.class_names = self.get_class_names()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
        "        img = Image.open(self.root / self.paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "    def read_file(self) -> Tuple[List[str], List[int]]:\n",
        "        paths = []\n",
        "        labels = []\n",
        "        with open(self.root / f\"{self.split}.txt\") as f:\n",
        "            for line in f:\n",
        "                path, _, label = line.strip().split(\", \")\n",
        "                paths.append(path)\n",
        "                labels.append(int(label))\n",
        "        return paths, labels\n",
        "\n",
        "    def get_class_names(self) -> List[str]:\n",
        "        class_names = {}\n",
        "        with open(self.root / \"train.txt\") as f:\n",
        "            for line in f:\n",
        "                path, _, label = line.strip().split(\", \")\n",
        "                if int(label) not in class_names:\n",
        "                    class_names[int(label)] = Path(path).parent.name\n",
        "        return [class_names[i] for i in range(len(class_names))]\n",
        "\n",
        "    def get_num_classes(self) -> int:\n",
        "        return max(self.labels) + 1\n",
        "\n",
        "# Sample dataset loader\n",
        "def sample_max_50(samples, dynamic_size=50):\n",
        "    sampled_samples = []\n",
        "    class_count = Counter([label for _, label in samples])\n",
        "    class_samples = {k: [] for k in class_count.keys()}\n",
        "\n",
        "    for img_path, label in samples:\n",
        "        class_samples[label].append((img_path, label))\n",
        "\n",
        "    for cls in class_samples.keys():\n",
        "        sampled_samples.extend(random.sample(class_samples[cls], min(dynamic_size, class_count[cls])))\n",
        "\n",
        "    return sampled_samples\n",
        "\n",
        "def filter_samples(samples, target_classes):\n",
        "    return [(img_path, label) for img_path, label in samples if label in target_classes]\n",
        "\n",
        "# Classes with high and low number of instances (as an example)\n",
        "#high_instance_classes = [0, 7, 13, 19, 20, 27, 38, 39, 41]\n",
        "#low_instance_classes = [10, 15, 24, 28, 30, 34, 40]\n",
        "high_instance_classes = [0, 19, 20, 27, 7, 41, 13, 38, 39]\n",
        "low_instance_classes = [24, 42, 11, 33, 14, 16, 34, 36, 15, 29, 10, 31, 35, 40, 28]\n",
        "l3 = high_instance_classes+low_instance_classes\n",
        "rest_instances = list(range(0,43))\n",
        "for i in l3:\n",
        "  rest_instances.remove(i)\n",
        "\n",
        "\n",
        "# Paths to dataset splits\n",
        "path2train = \"/kaggle/working/GroceryStoreDataset/dataset/train\"\n",
        "path2val = \"/kaggle/working/GroceryStoreDataset/dataset/val\"\n",
        "path2test = \"/kaggle/working/GroceryStoreDataset/dataset/test\"\n",
        "\n",
        "# Data transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset_full = GroceryStoreDataset(split='train', transform=data_transforms['train'])\n",
        "val_dataset_full = GroceryStoreDataset(split='val', transform=data_transforms['val'])\n",
        "test_dataset_full = GroceryStoreDataset(split='test', transform=data_transforms['test'])\n",
        "\n",
        "# Prepare samples\n",
        "train_samples = list(zip(train_dataset_full.paths, train_dataset_full.labels))\n",
        "val_samples = list(zip(val_dataset_full.paths, val_dataset_full.labels))\n",
        "\n",
        "train_samples_50 = sample_max_50(train_samples, 36)\n",
        "train_samples_high = filter_samples(train_samples, high_instance_classes)\n",
        "train_samples_low = filter_samples(train_samples, low_instance_classes)\n",
        "\n",
        "# Create datasets using the filtered samples\n",
        "train_dataset_50 = GroceryStoreDataset(samples=train_samples_50, transform=data_transforms['train'])\n",
        "train_dataset_high = GroceryStoreDataset(samples=train_samples_high, transform=data_transforms['train'])\n",
        "train_dataset_low = GroceryStoreDataset(samples=train_samples_low, transform=data_transforms['train'])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader_50 = DataLoader(train_dataset_50, batch_size=32, shuffle=True, num_workers=4)\n",
        "train_loader_high = DataLoader(train_dataset_high, batch_size=32, shuffle=True, num_workers=4)\n",
        "train_loader_low = DataLoader(train_dataset_low, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# Prepare validation samples\n",
        "val_samples_50 = sample_max_50(val_samples)\n",
        "val_samples_high = filter_samples(val_samples, high_instance_classes)\n",
        "val_samples_low = filter_samples(val_samples, low_instance_classes)\n",
        "\n",
        "# Create validation datasets using the filtered samples\n",
        "val_dataset_50 = GroceryStoreDataset(samples=val_samples_50, transform=data_transforms['val'])\n",
        "val_dataset_high = GroceryStoreDataset(samples=val_samples_high, transform=data_transforms['val'])\n",
        "val_dataset_low = GroceryStoreDataset(samples=val_samples_low, transform=data_transforms['val'])\n",
        "\n",
        "# Create validation DataLoaders\n",
        "val_loader_50 = DataLoader(val_dataset_50, batch_size=32, shuffle=False, num_workers=4)\n",
        "val_loader_high = DataLoader(val_dataset_high, batch_size=32, shuffle=False, num_workers=4)\n",
        "val_loader_low = DataLoader(val_dataset_low, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Now can proceed with training and evaluation as before\n",
        "train_dataset = GroceryStoreDataset(split='train', transform=data_transforms['train'])\n",
        "val_dataset = GroceryStoreDataset(split='val', transform=data_transforms['val'])\n",
        "test_dataset = GroceryStoreDataset(split='test', transform=data_transforms['test'])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T19:41:44.446543Z",
          "iopub.execute_input": "2024-07-01T19:41:44.446915Z",
          "iopub.status.idle": "2024-07-01T19:41:44.546050Z",
          "shell.execute_reply.started": "2024-07-01T19:41:44.446884Z",
          "shell.execute_reply": "2024-07-01T19:41:44.545050Z"
        },
        "trusted": true,
        "id": "9RkDdeW7TfOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l3 = high_instance_classes+low_instance_classes\n",
        "rest_instances = list(range(0,43))\n",
        "for i in l3:\n",
        "  rest_instances.remove(i)\n",
        "\n",
        "alternative_data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "train_samples_rest = filter_samples(train_samples, rest_instances)\n",
        "train_dataset_rest = GroceryStoreDataset(samples=train_samples_rest, transform=alternative_data_transforms['train'])\n",
        "train_loader_rest = DataLoader(train_dataset_rest, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_samples_rest = filter_samples(val_samples, rest_instances)\n",
        "val_dataset_rest = GroceryStoreDataset(samples=val_samples_rest, transform=alternative_data_transforms['val'])\n",
        "val_loader_rest = DataLoader(val_dataset_rest, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T20:10:21.789591Z",
          "iopub.execute_input": "2024-07-01T20:10:21.790086Z",
          "iopub.status.idle": "2024-07-01T20:10:21.813814Z",
          "shell.execute_reply.started": "2024-07-01T20:10:21.790048Z",
          "shell.execute_reply": "2024-07-01T20:10:21.812920Z"
        },
        "trusted": true,
        "id": "1KDjgSKvTfOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_copy = CustomDenseNet(1000)  # Replace with model class\n",
        "\n",
        "# Load the state_dict into the new model instance\n",
        "model_copy.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:37:54.578662Z",
          "iopub.execute_input": "2024-07-02T17:37:54.579020Z",
          "iopub.status.idle": "2024-07-02T17:37:54.807878Z",
          "shell.execute_reply.started": "2024-07-02T17:37:54.578990Z",
          "shell.execute_reply": "2024-07-02T17:37:54.806968Z"
        },
        "trusted": true,
        "id": "8AmgYy8wTfOL",
        "outputId": "7f7b9e11-dad8-468a-e9fc-652dd57f6823"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = CustomDenseNet(1000)  # Replace with model class\n",
        "\n",
        "# Load the state_dict into the new model instance\n",
        "model2.load_state_dict(torch.load('/kaggle/working/best_model.pth'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:38:02.418336Z",
          "iopub.execute_input": "2024-07-02T17:38:02.418951Z",
          "iopub.status.idle": "2024-07-02T17:38:02.643487Z",
          "shell.execute_reply.started": "2024-07-02T17:38:02.418922Z",
          "shell.execute_reply": "2024-07-02T17:38:02.642538Z"
        },
        "trusted": true,
        "id": "QxnEOSkbTfOL",
        "outputId": "4af37295-13c4-4936-d90d-55dc20f11d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = CustomDenseNet(1000)  # Replace with model class\n",
        "\n",
        "# Load the state_dict into the new model instance\n",
        "model3.load_state_dict(torch.load('/kaggle/working/best_model.pth'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:38:04.462557Z",
          "iopub.execute_input": "2024-07-02T17:38:04.462920Z",
          "iopub.status.idle": "2024-07-02T17:38:04.692776Z",
          "shell.execute_reply.started": "2024-07-02T17:38:04.462890Z",
          "shell.execute_reply": "2024-07-02T17:38:04.691879Z"
        },
        "trusted": true,
        "id": "r5nEBoTNTfOL",
        "outputId": "f968650b-31ba-445c-de6e-5d5de6eca4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copynn(model):\n",
        "    model_copy = CustomDenseNet()  # Create a new instance of model\n",
        "    model_copy.load_state_dict(model.state_dict())\n",
        "    return model_copy\n",
        "class EnsembleModelComplex(nn.Module):\n",
        "    def __init__(self, model_50, model_high, model_low, num_classes):\n",
        "        super(EnsembleModelComplex, self).__init__()\n",
        "        self.model_50 = model_50[0]\n",
        "        self.model_high = model_high[0]\n",
        "        self.model_low = model_low[0]\n",
        "\n",
        "        for param in self.model_50.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.model_high.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.model_low.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last 15 layers of each model\n",
        "        for param in list(self.model_50.parameters())[-15:]:\n",
        "            param.requires_grad = True\n",
        "        for param in list(self.model_high.parameters())[-15:]:\n",
        "            param.requires_grad = True\n",
        "        for param in list(self.model_low.parameters())[-15:]:\n",
        "            param.requires_grad = True\n",
        "\n",
        "\n",
        "        # Define a more complex combination network\n",
        "        self.fc1 = nn.Linear(num_classes * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_50 = self.model_50(x)\n",
        "        out_high = self.model_high(x)\n",
        "        out_low = self.model_low(x)\n",
        "\n",
        "        out = torch.cat((out_50, out_high, out_low), dim=1)\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.fc2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def train_ensemble_model_complex(ensemble_model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=5):\n",
        "    best_val_acc = 0.0\n",
        "    best_model_weights = None\n",
        "    no_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ensemble_model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = ensemble_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        val_acc = evaluate_model_complex(ensemble_model, val_loader)\n",
        "        print(f'Validation Accuracy: {val_acc:.4f}')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_weights = ensemble_model.state_dict().copy()\n",
        "            no_improvement = 0\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "\n",
        "        if no_improvement >= patience:\n",
        "            print('Early stopping due to no improvement')\n",
        "            break\n",
        "\n",
        "    if best_model_weights:\n",
        "        ensemble_model.load_state_dict(best_model_weights)\n",
        "\n",
        "    return ensemble_model\n",
        "\n",
        "def evaluate_model_complex(ensemble_model, val_loader):\n",
        "    ensemble_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = ensemble_model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T21:25:30.260587Z",
          "iopub.execute_input": "2024-07-01T21:25:30.261424Z",
          "iopub.status.idle": "2024-07-01T21:25:30.282336Z",
          "shell.execute_reply.started": "2024-07-01T21:25:30.261389Z",
          "shell.execute_reply": "2024-07-01T21:25:30.281367Z"
        },
        "trusted": true,
        "id": "a3mHGw4mTfON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "model_copy1 = copy.deepcopy(model_rest)\n",
        "model_copy2 = copy.deepcopy(model_high_dens)\n",
        "model_copy3 = copy.deepcopy(model_low_dens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T21:24:51.186071Z",
          "iopub.execute_input": "2024-07-01T21:24:51.186935Z",
          "iopub.status.idle": "2024-07-01T21:24:51.361310Z",
          "shell.execute_reply.started": "2024-07-01T21:24:51.186900Z",
          "shell.execute_reply": "2024-07-01T21:24:51.360480Z"
        },
        "trusted": true,
        "id": "HXvrGNiaTfON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 35\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_low_dens = model#CustomDenseNet(num_classes)\n",
        "model_low_dens = model_low_dens.to(device)\n",
        "optimizer_low = optim.AdamW(model_low_dens.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler_low = ReduceLROnPlateau(optimizer_low, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "#model_high = train_individual_model(model_high, train_loader_high, nn.CrossEntropyLoss(), optimizer_high, scheduler_high)\n",
        "model_low_dens = train_model_customCNN(model_low_dens, nn.CrossEntropyLoss(), optimizer_low, scheduler_low, train_loader_low, val_loader_low, num_epochs=epochs)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:38:46.418745Z",
          "iopub.execute_input": "2024-07-02T17:38:46.419110Z",
          "iopub.status.idle": "2024-07-02T17:39:30.885538Z",
          "shell.execute_reply.started": "2024-07-02T17:38:46.419081Z",
          "shell.execute_reply": "2024-07-02T17:39:30.884440Z"
        },
        "trusted": true,
        "id": "WiHA2ULkTfOS",
        "outputId": "784f5972-9f32-4c80-93cf-34293056c1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/35] Train Loss: 2.7519, Train Acc: 0.4046, Val Loss: 2.2759, Val Acc: 0.3654\nEpoch [2/35] Train Loss: 1.3356, Train Acc: 0.8840, Val Loss: 1.8725, Val Acc: 0.5000\nEpoch [3/35] Train Loss: 0.7086, Train Acc: 0.9407, Val Loss: 1.6482, Val Acc: 0.5192\nEpoch [4/35] Train Loss: 0.4226, Train Acc: 0.9820, Val Loss: 1.6437, Val Acc: 0.5385\nEpoch [5/35] Train Loss: 0.2764, Train Acc: 0.9768, Val Loss: 1.6792, Val Acc: 0.5385\nEpoch [6/35] Train Loss: 0.2120, Train Acc: 0.9794, Val Loss: 1.6762, Val Acc: 0.5192\nEpoch [7/35] Train Loss: 0.1739, Train Acc: 0.9845, Val Loss: 1.6851, Val Acc: 0.5577\nEpoch 00007: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [8/35] Train Loss: 0.1564, Train Acc: 0.9871, Val Loss: 1.6955, Val Acc: 0.5577\nEpoch [9/35] Train Loss: 0.1336, Train Acc: 1.0000, Val Loss: 1.7365, Val Acc: 0.5577\nEpoch [10/35] Train Loss: 0.1478, Train Acc: 0.9820, Val Loss: 1.7070, Val Acc: 0.5577\nEpoch [11/35] Train Loss: 0.1501, Train Acc: 0.9871, Val Loss: 1.7384, Val Acc: 0.5577\nEpoch [12/35] Train Loss: 0.1446, Train Acc: 0.9845, Val Loss: 1.7290, Val Acc: 0.5577\nEpoch [13/35] Train Loss: 0.1435, Train Acc: 0.9845, Val Loss: 1.7073, Val Acc: 0.5385\nEpoch 00013: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [14/35] Train Loss: 0.1365, Train Acc: 0.9897, Val Loss: 1.7372, Val Acc: 0.5577\nEpoch [15/35] Train Loss: 0.1184, Train Acc: 0.9974, Val Loss: 1.6870, Val Acc: 0.5577\nEpoch [16/35] Train Loss: 0.1516, Train Acc: 0.9794, Val Loss: 1.7129, Val Acc: 0.5577\nEpoch [17/35] Train Loss: 0.1415, Train Acc: 0.9871, Val Loss: 1.7337, Val Acc: 0.5769\nEpoch [18/35] Train Loss: 0.1210, Train Acc: 0.9871, Val Loss: 1.7599, Val Acc: 0.5577\nEpoch [19/35] Train Loss: 0.1605, Train Acc: 0.9871, Val Loss: 1.6795, Val Acc: 0.5577\nEarly stopping triggered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.2\n",
        "num_ftrs = model_copy.fc.in_features\n",
        "model2.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(num_ftrs, 43)\n",
        "            )\n",
        "epochs = 35\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_rest = model2.to(device)#CustomDenseNet(num_classes, growth_rate=32).to(device)\n",
        "optimizer_rest = optim.AdamW(model_rest.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler_rest = ReduceLROnPlateau(optimizer_rest, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "model_rest = train_model_customCNN(model_rest, nn.CrossEntropyLoss(), optimizer_rest, scheduler_rest, train_loader_rest, val_loader_rest, num_epochs=epochs, patience= 10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:39:48.271322Z",
          "iopub.execute_input": "2024-07-02T17:39:48.271711Z",
          "iopub.status.idle": "2024-07-02T17:42:03.768798Z",
          "shell.execute_reply.started": "2024-07-02T17:39:48.271677Z",
          "shell.execute_reply": "2024-07-02T17:42:03.767607Z"
        },
        "trusted": true,
        "id": "-qwkE3MHTfOT",
        "outputId": "56dec8e0-ef14-4969-9aa7-bd65cbc610f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/35] Train Loss: 2.8505, Train Acc: 0.2321, Val Loss: 2.3561, Val Acc: 0.1124\nEpoch [2/35] Train Loss: 1.8153, Train Acc: 0.4859, Val Loss: 1.8799, Val Acc: 0.3933\nEpoch [3/35] Train Loss: 1.2275, Train Acc: 0.6910, Val Loss: 1.6310, Val Acc: 0.5393\nEpoch [4/35] Train Loss: 0.8685, Train Acc: 0.7718, Val Loss: 1.3124, Val Acc: 0.5730\nEpoch [5/35] Train Loss: 0.6270, Train Acc: 0.8333, Val Loss: 1.3152, Val Acc: 0.5955\nEpoch [6/35] Train Loss: 0.4629, Train Acc: 0.8821, Val Loss: 1.2108, Val Acc: 0.6854\nEpoch [7/35] Train Loss: 0.3576, Train Acc: 0.9115, Val Loss: 1.2824, Val Acc: 0.6180\nEpoch 00007: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [8/35] Train Loss: 0.2701, Train Acc: 0.9282, Val Loss: 1.2309, Val Acc: 0.6292\nEpoch [9/35] Train Loss: 0.2609, Train Acc: 0.9436, Val Loss: 1.1780, Val Acc: 0.6629\nEpoch [10/35] Train Loss: 0.2500, Train Acc: 0.9449, Val Loss: 1.1999, Val Acc: 0.6966\nEpoch [11/35] Train Loss: 0.2083, Train Acc: 0.9551, Val Loss: 1.2407, Val Acc: 0.6854\nEpoch [12/35] Train Loss: 0.1939, Train Acc: 0.9590, Val Loss: 1.2160, Val Acc: 0.6742\nEpoch [13/35] Train Loss: 0.1835, Train Acc: 0.9654, Val Loss: 1.1970, Val Acc: 0.6742\nEpoch 00013: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [14/35] Train Loss: 0.1820, Train Acc: 0.9603, Val Loss: 1.1897, Val Acc: 0.6854\nEpoch [15/35] Train Loss: 0.1829, Train Acc: 0.9577, Val Loss: 1.1819, Val Acc: 0.6742\nEpoch [16/35] Train Loss: 0.1711, Train Acc: 0.9744, Val Loss: 1.1594, Val Acc: 0.7079\nEpoch [17/35] Train Loss: 0.1697, Train Acc: 0.9731, Val Loss: 1.2055, Val Acc: 0.6854\nEpoch [18/35] Train Loss: 0.1776, Train Acc: 0.9628, Val Loss: 1.1790, Val Acc: 0.6854\nEpoch [19/35] Train Loss: 0.1933, Train Acc: 0.9564, Val Loss: 1.2070, Val Acc: 0.6854\nEpoch 00019: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [20/35] Train Loss: 0.1795, Train Acc: 0.9667, Val Loss: 1.1949, Val Acc: 0.6742\nEpoch [21/35] Train Loss: 0.1947, Train Acc: 0.9487, Val Loss: 1.2042, Val Acc: 0.6629\nEpoch [22/35] Train Loss: 0.1930, Train Acc: 0.9577, Val Loss: 1.2128, Val Acc: 0.6854\nEpoch [23/35] Train Loss: 0.1999, Train Acc: 0.9538, Val Loss: 1.2069, Val Acc: 0.6854\nEpoch [24/35] Train Loss: 0.1784, Train Acc: 0.9679, Val Loss: 1.2109, Val Acc: 0.6742\nEpoch [25/35] Train Loss: 0.1866, Train Acc: 0.9628, Val Loss: 1.1692, Val Acc: 0.6966\nEpoch 00025: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [26/35] Train Loss: 0.2052, Train Acc: 0.9564, Val Loss: 1.1661, Val Acc: 0.7191\nEarly stopping triggered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.2\n",
        "num_ftrs = model_copy.fc.in_features\n",
        "model3.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(num_ftrs, 43)\n",
        "            )\n",
        "#model_high_dens = CustomDenseNet(num_classes)\n",
        "model_high_dens = model3.to(device)#model_high_dens.to(device)\n",
        "optimizer_high = optim.AdamW(model_high_dens.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler_high = ReduceLROnPlateau(optimizer_high, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "#model_high = train_individual_model(model_high, train_loader_high, nn.CrossEntropyLoss(), optimizer_high, scheduler_high)\n",
        "model_high_dens = train_model_customCNN(model_high_dens, nn.CrossEntropyLoss(), optimizer_high, scheduler_high, train_loader_high, val_loader_high, num_epochs=epochs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:42:03.770774Z",
          "iopub.execute_input": "2024-07-02T17:42:03.771111Z",
          "iopub.status.idle": "2024-07-02T17:45:36.144340Z",
          "shell.execute_reply.started": "2024-07-02T17:42:03.771076Z",
          "shell.execute_reply": "2024-07-02T17:45:36.143183Z"
        },
        "trusted": true,
        "id": "bzl9qE4ETfOT",
        "outputId": "cf0dcc5a-b6a4-40c2-9c9c-616ec9617fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/35] Train Loss: 1.8865, Train Acc: 0.4130, Val Loss: 1.0590, Val Acc: 0.6065\nEpoch [2/35] Train Loss: 0.7999, Train Acc: 0.7391, Val Loss: 0.6596, Val Acc: 0.7548\nEpoch [3/35] Train Loss: 0.4710, Train Acc: 0.8471, Val Loss: 0.6752, Val Acc: 0.7742\nEpoch [4/35] Train Loss: 0.3643, Train Acc: 0.8838, Val Loss: 0.5360, Val Acc: 0.8129\nEpoch [5/35] Train Loss: 0.2404, Train Acc: 0.9198, Val Loss: 0.6884, Val Acc: 0.7548\nEpoch [6/35] Train Loss: 0.2076, Train Acc: 0.9307, Val Loss: 0.8718, Val Acc: 0.7484\nEpoch [7/35] Train Loss: 0.1492, Train Acc: 0.9524, Val Loss: 0.5193, Val Acc: 0.8194\nEpoch 00007: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [8/35] Train Loss: 0.1445, Train Acc: 0.9572, Val Loss: 0.4530, Val Acc: 0.8194\nEpoch [9/35] Train Loss: 0.1084, Train Acc: 0.9681, Val Loss: 0.4404, Val Acc: 0.8258\nEpoch [10/35] Train Loss: 0.0917, Train Acc: 0.9789, Val Loss: 0.4674, Val Acc: 0.8194\nEpoch [11/35] Train Loss: 0.0899, Train Acc: 0.9715, Val Loss: 0.4965, Val Acc: 0.8323\nEpoch [12/35] Train Loss: 0.0833, Train Acc: 0.9823, Val Loss: 0.4857, Val Acc: 0.8129\nEpoch [13/35] Train Loss: 0.0720, Train Acc: 0.9823, Val Loss: 0.5150, Val Acc: 0.8323\nEpoch 00013: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [14/35] Train Loss: 0.0800, Train Acc: 0.9783, Val Loss: 0.4749, Val Acc: 0.8581\nEpoch [15/35] Train Loss: 0.0781, Train Acc: 0.9837, Val Loss: 0.4905, Val Acc: 0.8516\nEpoch [16/35] Train Loss: 0.0704, Train Acc: 0.9823, Val Loss: 0.5153, Val Acc: 0.8323\nEpoch [17/35] Train Loss: 0.0650, Train Acc: 0.9871, Val Loss: 0.4585, Val Acc: 0.8323\nEpoch [18/35] Train Loss: 0.0691, Train Acc: 0.9851, Val Loss: 0.4668, Val Acc: 0.8581\nEpoch [19/35] Train Loss: 0.0725, Train Acc: 0.9796, Val Loss: 0.4807, Val Acc: 0.8516\nEpoch 00019: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [20/35] Train Loss: 0.0689, Train Acc: 0.9857, Val Loss: 0.4708, Val Acc: 0.8516\nEpoch [21/35] Train Loss: 0.0718, Train Acc: 0.9837, Val Loss: 0.4936, Val Acc: 0.8194\nEpoch [22/35] Train Loss: 0.0667, Train Acc: 0.9830, Val Loss: 0.4834, Val Acc: 0.8581\nEpoch [23/35] Train Loss: 0.0583, Train Acc: 0.9857, Val Loss: 0.4864, Val Acc: 0.8516\nEpoch [24/35] Train Loss: 0.0711, Train Acc: 0.9817, Val Loss: 0.4935, Val Acc: 0.8516\nEarly stopping triggered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FINAL RESULT FOR PART1"
      ],
      "metadata": {
        "id": "e9xvRrc5Zq2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the ensemble model\n",
        "ensemble_model_complex = EnsembleModelComplex(model_rest, model_high_dens, model_low_dens, num_classes).to(device)\n",
        "\n",
        "# Define loss, optimizer, and scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ensemble = optim.Adam(ensemble_model_complex.parameters(), lr=0.001)  # Optimize all parameters in the complex model\n",
        "scheduler_ensemble = optim.lr_scheduler.StepLR(optimizer_ensemble, step_size=5, gamma=0.5)\n",
        "\n",
        "# Train the ensemble model\n",
        "ensemble_model_complex = train_ensemble_model_complex(ensemble_model_complex, train_loader, val_loader, criterion, optimizer_ensemble, scheduler_ensemble, patience= 10)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:45:36.146299Z",
          "iopub.execute_input": "2024-07-02T17:45:36.146627Z",
          "iopub.status.idle": "2024-07-02T17:56:04.134719Z",
          "shell.execute_reply.started": "2024-07-02T17:45:36.146592Z",
          "shell.execute_reply": "2024-07-02T17:56:04.133605Z"
        },
        "trusted": true,
        "id": "_scnDbIjTfOT",
        "outputId": "8025fa65-7122-4c4b-d31c-5f00eb6491e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/25] Loss: 2.6273, Accuracy: 0.3398\nValidation Accuracy: 0.4324\nEpoch [2/25] Loss: 1.5975, Accuracy: 0.5545\nValidation Accuracy: 0.4865\nEpoch [3/25] Loss: 1.1138, Accuracy: 0.6663\nValidation Accuracy: 0.5405\nEpoch [4/25] Loss: 0.9337, Accuracy: 0.7318\nValidation Accuracy: 0.5473\nEpoch [5/25] Loss: 0.8208, Accuracy: 0.7705\nValidation Accuracy: 0.5034\nEpoch [6/25] Loss: 0.5930, Accuracy: 0.8178\nValidation Accuracy: 0.5405\nEpoch [7/25] Loss: 0.5146, Accuracy: 0.8515\nValidation Accuracy: 0.5845\nEpoch [8/25] Loss: 0.4305, Accuracy: 0.8712\nValidation Accuracy: 0.5676\nEpoch [9/25] Loss: 0.4347, Accuracy: 0.8716\nValidation Accuracy: 0.5811\nEpoch [10/25] Loss: 0.3956, Accuracy: 0.8837\nValidation Accuracy: 0.5946\nEpoch [11/25] Loss: 0.3512, Accuracy: 0.8947\nValidation Accuracy: 0.5743\nEpoch [12/25] Loss: 0.3113, Accuracy: 0.9045\nValidation Accuracy: 0.6081\nEpoch [13/25] Loss: 0.3021, Accuracy: 0.9000\nValidation Accuracy: 0.5743\nEpoch [14/25] Loss: 0.3266, Accuracy: 0.8992\nValidation Accuracy: 0.5878\nEpoch [15/25] Loss: 0.2789, Accuracy: 0.9155\nValidation Accuracy: 0.6081\nEpoch [16/25] Loss: 0.2575, Accuracy: 0.9231\nValidation Accuracy: 0.6014\nEpoch [17/25] Loss: 0.2439, Accuracy: 0.9284\nValidation Accuracy: 0.6047\nEpoch [18/25] Loss: 0.2509, Accuracy: 0.9197\nValidation Accuracy: 0.5743\nEpoch [19/25] Loss: 0.2374, Accuracy: 0.9284\nValidation Accuracy: 0.5777\nEpoch [20/25] Loss: 0.2169, Accuracy: 0.9303\nValidation Accuracy: 0.5878\nEpoch [21/25] Loss: 0.2191, Accuracy: 0.9277\nValidation Accuracy: 0.5878\nEpoch [22/25] Loss: 0.2163, Accuracy: 0.9295\nValidation Accuracy: 0.6047\nEarly stopping due to no improvement\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PART2 FINAL RESULT"
      ],
      "metadata": {
        "id": "PcmMDgt5ZvyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the model\n",
        "def create_model():\n",
        "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, 43)  # Adjust the final layer to the number of classes\n",
        "    return model.to(device)\n",
        "\n",
        "train_labels = [label for _, label in train_samples]\n",
        "# Assuming train_labels contains all the labels from training dataset\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weights = torch.FloatTensor(class_weights).to(device)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Mixup augmentation\n",
        "def mixup_data(x, y, alpha=0.2):\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "# Early stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, min_delta=0):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "        self.best_model_wts = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model_wts = model.state_dict()\n",
        "        elif val_loss > self.best_loss - self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.best_model_wts = model.state_dict()\n",
        "            self.counter = 0\n",
        "\n",
        "    def get_best_model_wts(self):\n",
        "        return self.best_model_wts\n",
        "\n",
        "# Training function\n",
        "def train_model(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=25, patience=15):\n",
        "    best_val_acc = 0.0\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    early_stopping = EarlyStopping(patience=patience, min_delta=0.01)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Apply Mixup\n",
        "            inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha= 0.2)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (lam * predicted.eq(targets_a).sum().item() + (1 - lam) * predicted.eq(targets_b).sum().item())\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = val_correct / val_total\n",
        "        val_losses.append(val_epoch_loss)\n",
        "        val_accuracies.append(val_epoch_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
        "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "        early_stopping(val_epoch_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            model.load_state_dict(early_stopping.get_best_model_wts())\n",
        "            break\n",
        "\n",
        "        if val_epoch_acc > best_val_acc:\n",
        "            best_val_acc = val_epoch_acc\n",
        "            best_model_wts = model.state_dict()\n",
        "\n",
        "        if isinstance(scheduler, ReduceLROnPlateau):\n",
        "            scheduler.step(val_epoch_loss)  # Pass validation loss to scheduler\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "# Hyperparameter configurations\n",
        "learning_rates = [0.0001, 0.0003]\n",
        "weight_decays = [1e-2, 1e-4]\n",
        "schedulers = [\n",
        "    lambda opt: ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=3, verbose=True),\n",
        "    lambda opt: CosineAnnealingLR(opt, T_max=10, eta_min=1e-6)\n",
        "]\n",
        "\n",
        "# Store the results\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for wd in weight_decays:\n",
        "        for scheduler_fn in schedulers:\n",
        "            print(f\"Training with lr={lr}, weight_decay={wd}\")\n",
        "            model = create_model()\n",
        "            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "            scheduler = scheduler_fn(optimizer)\n",
        "\n",
        "            model, train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
        "                model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=25, patience=15\n",
        "            )\n",
        "\n",
        "            results.append({\n",
        "                'lr': lr,\n",
        "                'weight_decay': wd,\n",
        "                'scheduler': scheduler.__class__.__name__,\n",
        "                'val_acc': max(val_accuracies),\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'train_accuracies': train_accuracies,\n",
        "                'val_accuracies': val_accuracies\n",
        "            })\n",
        "\n",
        "# Display the results\n",
        "import ace_tools as tools; tools.display_dataframe_to_user(\"Model Training Results\", pd.DataFrame(results))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-15T07:40:07.701295Z",
          "iopub.execute_input": "2024-07-15T07:40:07.701666Z",
          "iopub.status.idle": "2024-07-15T08:28:29.190519Z",
          "shell.execute_reply.started": "2024-07-15T07:40:07.701639Z",
          "shell.execute_reply": "2024-07-15T08:28:29.189178Z"
        },
        "trusted": true,
        "id": "vkHe7_crX10u",
        "outputId": "3698ed92-00e0-4e06-b192-5bb781c0b026"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Training with lr=0.0001, weight_decay=0.01\nEpoch [1/25] Train Loss: 2.3917, Train Acc: 0.3796, Val Loss: 1.3890, Val Acc: 0.6284\nEpoch [2/25] Train Loss: 1.1664, Train Acc: 0.7305, Val Loss: 1.0612, Val Acc: 0.7331\nEpoch [3/25] Train Loss: 0.8913, Train Acc: 0.8102, Val Loss: 1.1420, Val Acc: 0.6959\nEpoch [4/25] Train Loss: 0.8082, Train Acc: 0.8482, Val Loss: 1.0301, Val Acc: 0.7500\nEpoch [5/25] Train Loss: 0.7778, Train Acc: 0.8481, Val Loss: 0.9606, Val Acc: 0.7804\nEpoch [6/25] Train Loss: 0.7179, Train Acc: 0.8653, Val Loss: 0.9126, Val Acc: 0.7804\nEpoch [7/25] Train Loss: 0.6542, Train Acc: 0.8881, Val Loss: 0.9475, Val Acc: 0.7669\nEpoch [8/25] Train Loss: 0.5650, Train Acc: 0.9072, Val Loss: 0.9913, Val Acc: 0.7601\nEpoch [9/25] Train Loss: 0.6719, Train Acc: 0.8848, Val Loss: 1.1161, Val Acc: 0.7432\nEpoch [10/25] Train Loss: 0.6335, Train Acc: 0.8952, Val Loss: 1.0516, Val Acc: 0.7770\nEpoch 00010: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [11/25] Train Loss: 0.6001, Train Acc: 0.9069, Val Loss: 0.9331, Val Acc: 0.7770\nEpoch [12/25] Train Loss: 0.6115, Train Acc: 0.9026, Val Loss: 0.9284, Val Acc: 0.7804\nEpoch [13/25] Train Loss: 0.6059, Train Acc: 0.8998, Val Loss: 0.9024, Val Acc: 0.7973\nEpoch [14/25] Train Loss: 0.7213, Train Acc: 0.8700, Val Loss: 0.9399, Val Acc: 0.7838\nEpoch [15/25] Train Loss: 0.6308, Train Acc: 0.8959, Val Loss: 0.9033, Val Acc: 0.8041\nEpoch [16/25] Train Loss: 0.5760, Train Acc: 0.9103, Val Loss: 0.9327, Val Acc: 0.8007\nEpoch [17/25] Train Loss: 0.5356, Train Acc: 0.9125, Val Loss: 0.9019, Val Acc: 0.7905\nEpoch [18/25] Train Loss: 0.6389, Train Acc: 0.8931, Val Loss: 0.9659, Val Acc: 0.8007\nEpoch [19/25] Train Loss: 0.6133, Train Acc: 0.9014, Val Loss: 0.9485, Val Acc: 0.7939\nEpoch [20/25] Train Loss: 0.6846, Train Acc: 0.8771, Val Loss: 0.9417, Val Acc: 0.7939\nEpoch [21/25] Train Loss: 0.5237, Train Acc: 0.9163, Val Loss: 0.8676, Val Acc: 0.8176\nEpoch [22/25] Train Loss: 0.6465, Train Acc: 0.8871, Val Loss: 0.9134, Val Acc: 0.7973\nEpoch [23/25] Train Loss: 0.5696, Train Acc: 0.9003, Val Loss: 0.9258, Val Acc: 0.8041\nEpoch [24/25] Train Loss: 0.4759, Train Acc: 0.9219, Val Loss: 0.8818, Val Acc: 0.8074\nEpoch [25/25] Train Loss: 0.5043, Train Acc: 0.9222, Val Loss: 0.9428, Val Acc: 0.8074\nEpoch 00025: reducing learning rate of group 0 to 1.0000e-06.\nTraining with lr=0.0001, weight_decay=0.01\nEpoch [1/25] Train Loss: 2.4183, Train Acc: 0.3855, Val Loss: 1.5929, Val Acc: 0.6047\nEpoch [2/25] Train Loss: 1.1559, Train Acc: 0.7299, Val Loss: 1.1219, Val Acc: 0.7162\nEpoch [3/25] Train Loss: 0.8486, Train Acc: 0.8118, Val Loss: 0.9236, Val Acc: 0.7703\nEpoch [4/25] Train Loss: 0.7947, Train Acc: 0.8362, Val Loss: 0.9294, Val Acc: 0.7905\nEpoch [5/25] Train Loss: 0.8438, Train Acc: 0.8381, Val Loss: 0.9703, Val Acc: 0.7635\nEpoch [6/25] Train Loss: 0.7825, Train Acc: 0.8602, Val Loss: 0.8501, Val Acc: 0.7838\nEpoch [7/25] Train Loss: 0.6276, Train Acc: 0.8932, Val Loss: 0.8564, Val Acc: 0.8007\nEpoch [8/25] Train Loss: 0.6039, Train Acc: 0.9005, Val Loss: 0.8083, Val Acc: 0.8108\nEpoch [9/25] Train Loss: 0.6036, Train Acc: 0.9029, Val Loss: 0.8203, Val Acc: 0.8209\nEpoch [10/25] Train Loss: 0.7352, Train Acc: 0.8750, Val Loss: 0.8738, Val Acc: 0.8007\nEpoch [11/25] Train Loss: 0.5786, Train Acc: 0.9081, Val Loss: 0.7987, Val Acc: 0.8176\nEpoch [12/25] Train Loss: 0.6941, Train Acc: 0.8807, Val Loss: 0.8266, Val Acc: 0.8041\nEpoch [13/25] Train Loss: 0.6493, Train Acc: 0.8850, Val Loss: 0.8680, Val Acc: 0.8041\nEpoch [14/25] Train Loss: 0.5971, Train Acc: 0.9058, Val Loss: 0.8160, Val Acc: 0.8277\nEpoch [15/25] Train Loss: 0.6553, Train Acc: 0.8902, Val Loss: 0.8486, Val Acc: 0.8142\nEpoch [16/25] Train Loss: 0.5837, Train Acc: 0.9013, Val Loss: 0.8332, Val Acc: 0.8074\nEpoch [17/25] Train Loss: 0.6408, Train Acc: 0.8886, Val Loss: 0.8884, Val Acc: 0.7838\nEpoch [18/25] Train Loss: 0.6019, Train Acc: 0.9013, Val Loss: 1.0202, Val Acc: 0.7331\nEpoch [19/25] Train Loss: 0.7147, Train Acc: 0.8701, Val Loss: 1.1002, Val Acc: 0.7196\nEpoch [20/25] Train Loss: 0.7842, Train Acc: 0.8617, Val Loss: 1.0806, Val Acc: 0.7297\nEpoch [21/25] Train Loss: 0.6842, Train Acc: 0.8795, Val Loss: 1.1708, Val Acc: 0.7027\nEpoch [22/25] Train Loss: 0.6081, Train Acc: 0.8924, Val Loss: 1.2771, Val Acc: 0.7297\nEpoch [23/25] Train Loss: 0.6124, Train Acc: 0.8907, Val Loss: 1.0813, Val Acc: 0.7500\nEarly stopping triggered\nTraining with lr=0.0001, weight_decay=0.0001\nEpoch [1/25] Train Loss: 2.3765, Train Acc: 0.4341, Val Loss: 1.4409, Val Acc: 0.6216\nEpoch [2/25] Train Loss: 1.1758, Train Acc: 0.7203, Val Loss: 1.3674, Val Acc: 0.6791\nEpoch [3/25] Train Loss: 0.8254, Train Acc: 0.8251, Val Loss: 1.2586, Val Acc: 0.6926\nEpoch [4/25] Train Loss: 0.7952, Train Acc: 0.8481, Val Loss: 1.0779, Val Acc: 0.7128\nEpoch [5/25] Train Loss: 0.7723, Train Acc: 0.8506, Val Loss: 0.9803, Val Acc: 0.7669\nEpoch [6/25] Train Loss: 0.7595, Train Acc: 0.8765, Val Loss: 0.9320, Val Acc: 0.7905\nEpoch [7/25] Train Loss: 0.7399, Train Acc: 0.8648, Val Loss: 1.1955, Val Acc: 0.7500\nEpoch [8/25] Train Loss: 0.8187, Train Acc: 0.8588, Val Loss: 1.0635, Val Acc: 0.7635\nEpoch [9/25] Train Loss: 0.5788, Train Acc: 0.9000, Val Loss: 1.0916, Val Acc: 0.7568\nEpoch [10/25] Train Loss: 0.7234, Train Acc: 0.8745, Val Loss: 1.0687, Val Acc: 0.7804\nEpoch 00010: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [11/25] Train Loss: 0.6151, Train Acc: 0.9047, Val Loss: 0.9882, Val Acc: 0.7635\nEpoch [12/25] Train Loss: 0.5978, Train Acc: 0.8973, Val Loss: 0.9880, Val Acc: 0.7635\nEpoch [13/25] Train Loss: 0.6022, Train Acc: 0.9006, Val Loss: 0.9261, Val Acc: 0.7905\nEpoch [14/25] Train Loss: 0.5932, Train Acc: 0.9033, Val Loss: 0.9672, Val Acc: 0.7669\nEpoch [15/25] Train Loss: 0.6605, Train Acc: 0.8930, Val Loss: 0.9643, Val Acc: 0.7838\nEpoch [16/25] Train Loss: 0.6470, Train Acc: 0.8933, Val Loss: 0.9938, Val Acc: 0.7804\nEpoch [17/25] Train Loss: 0.6306, Train Acc: 0.8974, Val Loss: 1.0053, Val Acc: 0.7703\nEpoch 00017: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [18/25] Train Loss: 0.5886, Train Acc: 0.8968, Val Loss: 0.9556, Val Acc: 0.7703\nEpoch [19/25] Train Loss: 0.6204, Train Acc: 0.8936, Val Loss: 0.9634, Val Acc: 0.7872\nEpoch [20/25] Train Loss: 0.5766, Train Acc: 0.9079, Val Loss: 0.9656, Val Acc: 0.7770\nEpoch [21/25] Train Loss: 0.6147, Train Acc: 0.8952, Val Loss: 0.9488, Val Acc: 0.7736\nEarly stopping triggered\nTraining with lr=0.0001, weight_decay=0.0001\nEpoch [1/25] Train Loss: 2.4027, Train Acc: 0.3546, Val Loss: 1.6159, Val Acc: 0.5169\nEpoch [2/25] Train Loss: 1.1884, Train Acc: 0.7157, Val Loss: 1.1200, Val Acc: 0.6959\nEpoch [3/25] Train Loss: 0.8796, Train Acc: 0.8159, Val Loss: 1.0979, Val Acc: 0.7061\nEpoch [4/25] Train Loss: 0.7635, Train Acc: 0.8470, Val Loss: 0.9995, Val Acc: 0.7399\nEpoch [5/25] Train Loss: 0.7134, Train Acc: 0.8688, Val Loss: 1.0392, Val Acc: 0.7500\nEpoch [6/25] Train Loss: 0.7125, Train Acc: 0.8802, Val Loss: 1.1750, Val Acc: 0.7128\nEpoch [7/25] Train Loss: 0.5908, Train Acc: 0.9044, Val Loss: 0.9539, Val Acc: 0.7534\nEpoch [8/25] Train Loss: 0.5843, Train Acc: 0.9140, Val Loss: 1.0444, Val Acc: 0.7669\nEpoch [9/25] Train Loss: 0.6097, Train Acc: 0.8968, Val Loss: 0.9844, Val Acc: 0.7770\nEpoch [10/25] Train Loss: 0.7182, Train Acc: 0.8689, Val Loss: 0.9926, Val Acc: 0.7770\nEpoch [11/25] Train Loss: 0.5596, Train Acc: 0.9029, Val Loss: 0.9792, Val Acc: 0.7703\nEpoch [12/25] Train Loss: 0.6736, Train Acc: 0.8824, Val Loss: 0.9767, Val Acc: 0.7770\nEpoch [13/25] Train Loss: 0.5995, Train Acc: 0.8989, Val Loss: 0.9854, Val Acc: 0.7939\nEpoch [14/25] Train Loss: 0.6129, Train Acc: 0.9079, Val Loss: 0.9896, Val Acc: 0.7905\nEpoch [15/25] Train Loss: 0.5799, Train Acc: 0.9030, Val Loss: 0.9389, Val Acc: 0.7905\nEpoch [16/25] Train Loss: 0.5614, Train Acc: 0.9113, Val Loss: 1.0167, Val Acc: 0.7770\nEpoch [17/25] Train Loss: 0.6461, Train Acc: 0.8872, Val Loss: 1.0188, Val Acc: 0.7669\nEpoch [18/25] Train Loss: 0.6693, Train Acc: 0.8868, Val Loss: 1.1063, Val Acc: 0.7703\nEpoch [19/25] Train Loss: 0.7921, Train Acc: 0.8661, Val Loss: 1.2493, Val Acc: 0.7736\nEpoch [20/25] Train Loss: 0.6736, Train Acc: 0.8817, Val Loss: 1.2789, Val Acc: 0.6824\nEpoch [21/25] Train Loss: 0.6994, Train Acc: 0.8863, Val Loss: 0.9653, Val Acc: 0.7804\nEpoch [22/25] Train Loss: 0.6265, Train Acc: 0.8953, Val Loss: 0.9703, Val Acc: 0.7736\nEpoch [23/25] Train Loss: 0.6759, Train Acc: 0.8800, Val Loss: 1.1951, Val Acc: 0.7399\nEpoch [24/25] Train Loss: 0.6230, Train Acc: 0.8973, Val Loss: 0.9494, Val Acc: 0.7770\nEpoch [25/25] Train Loss: 0.5953, Train Acc: 0.8972, Val Loss: 0.8851, Val Acc: 0.8007\nTraining with lr=0.0003, weight_decay=0.01\nEpoch [1/25] Train Loss: 2.2519, Train Acc: 0.4183, Val Loss: 1.8608, Val Acc: 0.5034\nEpoch [2/25] Train Loss: 1.3306, Train Acc: 0.6554, Val Loss: 1.5467, Val Acc: 0.6655\nEpoch [3/25] Train Loss: 0.9652, Train Acc: 0.7815, Val Loss: 1.8997, Val Acc: 0.5372\nEpoch [4/25] Train Loss: 0.9287, Train Acc: 0.7958, Val Loss: 2.2108, Val Acc: 0.5101\nEpoch [5/25] Train Loss: 1.0304, Train Acc: 0.7843, Val Loss: 1.6593, Val Acc: 0.5473\nEpoch [6/25] Train Loss: 0.8464, Train Acc: 0.8280, Val Loss: 1.5870, Val Acc: 0.6486\nEpoch 00006: reducing learning rate of group 0 to 3.0000e-05.\nEpoch [7/25] Train Loss: 0.8323, Train Acc: 0.8401, Val Loss: 1.1316, Val Acc: 0.7095\nEpoch [8/25] Train Loss: 0.8163, Train Acc: 0.8591, Val Loss: 1.0957, Val Acc: 0.7331\nEpoch [9/25] Train Loss: 0.6595, Train Acc: 0.8887, Val Loss: 1.0542, Val Acc: 0.7331\nEpoch [10/25] Train Loss: 0.6984, Train Acc: 0.8825, Val Loss: 1.0579, Val Acc: 0.7466\nEpoch [11/25] Train Loss: 0.6560, Train Acc: 0.8906, Val Loss: 1.1060, Val Acc: 0.7196\nEpoch [12/25] Train Loss: 0.6273, Train Acc: 0.8911, Val Loss: 1.0935, Val Acc: 0.7365\nEpoch [13/25] Train Loss: 0.6173, Train Acc: 0.8993, Val Loss: 1.0913, Val Acc: 0.7264\nEpoch 00013: reducing learning rate of group 0 to 3.0000e-06.\nEpoch [14/25] Train Loss: 0.6693, Train Acc: 0.8943, Val Loss: 1.1370, Val Acc: 0.7297\nEpoch [15/25] Train Loss: 0.6265, Train Acc: 0.8972, Val Loss: 1.1108, Val Acc: 0.7196\nEpoch [16/25] Train Loss: 0.6556, Train Acc: 0.8918, Val Loss: 1.0923, Val Acc: 0.7230\nEpoch [17/25] Train Loss: 0.5861, Train Acc: 0.9017, Val Loss: 1.1187, Val Acc: 0.7196\nEpoch 00017: reducing learning rate of group 0 to 3.0000e-07.\nEpoch [18/25] Train Loss: 0.6066, Train Acc: 0.9005, Val Loss: 1.0797, Val Acc: 0.7432\nEpoch [19/25] Train Loss: 0.6174, Train Acc: 0.8962, Val Loss: 1.1205, Val Acc: 0.7230\nEpoch [20/25] Train Loss: 0.6173, Train Acc: 0.8956, Val Loss: 1.1152, Val Acc: 0.7365\nEpoch [21/25] Train Loss: 0.7251, Train Acc: 0.8826, Val Loss: 1.0978, Val Acc: 0.7264\nEpoch 00021: reducing learning rate of group 0 to 3.0000e-08.\nEpoch [22/25] Train Loss: 0.5792, Train Acc: 0.9117, Val Loss: 1.0705, Val Acc: 0.7365\nEpoch [23/25] Train Loss: 0.6363, Train Acc: 0.8932, Val Loss: 1.1026, Val Acc: 0.7399\nEpoch [24/25] Train Loss: 0.5903, Train Acc: 0.8996, Val Loss: 1.0855, Val Acc: 0.7297\nEarly stopping triggered\nTraining with lr=0.0003, weight_decay=0.01\nEpoch [1/25] Train Loss: 2.2011, Train Acc: 0.4346, Val Loss: 1.6804, Val Acc: 0.4932\nEpoch [2/25] Train Loss: 1.3425, Train Acc: 0.6688, Val Loss: 1.9712, Val Acc: 0.4426\nEpoch [3/25] Train Loss: 1.1119, Train Acc: 0.7383, Val Loss: 1.5947, Val Acc: 0.6149\nEpoch [4/25] Train Loss: 0.8962, Train Acc: 0.8027, Val Loss: 1.2216, Val Acc: 0.6757\nEpoch [5/25] Train Loss: 0.8337, Train Acc: 0.8348, Val Loss: 1.3256, Val Acc: 0.6453\nEpoch [6/25] Train Loss: 0.6932, Train Acc: 0.8760, Val Loss: 1.0476, Val Acc: 0.7399\nEpoch [7/25] Train Loss: 0.7241, Train Acc: 0.8749, Val Loss: 1.2594, Val Acc: 0.6926\nEpoch [8/25] Train Loss: 0.6893, Train Acc: 0.8774, Val Loss: 1.1949, Val Acc: 0.7162\nEpoch [9/25] Train Loss: 0.6525, Train Acc: 0.8868, Val Loss: 1.0452, Val Acc: 0.7635\nEpoch [10/25] Train Loss: 0.6404, Train Acc: 0.8879, Val Loss: 0.9664, Val Acc: 0.7736\nEpoch [11/25] Train Loss: 0.5427, Train Acc: 0.9149, Val Loss: 1.0352, Val Acc: 0.7601\nEpoch [12/25] Train Loss: 0.5905, Train Acc: 0.9002, Val Loss: 1.0291, Val Acc: 0.7601\nEpoch [13/25] Train Loss: 0.6678, Train Acc: 0.8932, Val Loss: 1.1622, Val Acc: 0.7466\nEpoch [14/25] Train Loss: 0.6399, Train Acc: 0.8920, Val Loss: 1.0142, Val Acc: 0.7669\nEpoch [15/25] Train Loss: 0.5868, Train Acc: 0.9034, Val Loss: 1.2703, Val Acc: 0.6993\nEpoch [16/25] Train Loss: 0.6302, Train Acc: 0.8883, Val Loss: 1.3795, Val Acc: 0.7230\nEpoch [17/25] Train Loss: 0.7951, Train Acc: 0.8603, Val Loss: 1.2171, Val Acc: 0.7230\nEpoch [18/25] Train Loss: 0.8995, Train Acc: 0.8288, Val Loss: 1.3378, Val Acc: 0.6892\nEpoch [19/25] Train Loss: 0.6512, Train Acc: 0.8816, Val Loss: 1.6694, Val Acc: 0.6047\nEpoch [20/25] Train Loss: 0.8685, Train Acc: 0.8380, Val Loss: 1.5650, Val Acc: 0.6351\nEpoch [21/25] Train Loss: 0.8665, Train Acc: 0.8377, Val Loss: 1.5243, Val Acc: 0.6520\nEpoch [22/25] Train Loss: 0.8100, Train Acc: 0.8482, Val Loss: 1.4265, Val Acc: 0.6858\nEpoch [23/25] Train Loss: 0.7881, Train Acc: 0.8544, Val Loss: 2.1610, Val Acc: 0.6284\nEpoch [24/25] Train Loss: 0.7864, Train Acc: 0.8588, Val Loss: 1.5500, Val Acc: 0.6588\nEpoch [25/25] Train Loss: 0.7311, Train Acc: 0.8527, Val Loss: 1.6501, Val Acc: 0.7128\nEarly stopping triggered\nTraining with lr=0.0003, weight_decay=0.0001\nEpoch [1/25] Train Loss: 2.1738, Train Acc: 0.4245, Val Loss: 1.4355, Val Acc: 0.5946\nEpoch [2/25] Train Loss: 1.3602, Train Acc: 0.6648, Val Loss: 2.1192, Val Acc: 0.5101\nEpoch [3/25] Train Loss: 1.1275, Train Acc: 0.7442, Val Loss: 1.5850, Val Acc: 0.5777\nEpoch [4/25] Train Loss: 0.9270, Train Acc: 0.7870, Val Loss: 1.7209, Val Acc: 0.5507\nEpoch [5/25] Train Loss: 0.9454, Train Acc: 0.7979, Val Loss: 1.7741, Val Acc: 0.5946\nEpoch 00005: reducing learning rate of group 0 to 3.0000e-05.\nEpoch [6/25] Train Loss: 0.7385, Train Acc: 0.8707, Val Loss: 1.3334, Val Acc: 0.6959\nEpoch [7/25] Train Loss: 0.6225, Train Acc: 0.8978, Val Loss: 1.2251, Val Acc: 0.7264\nEpoch [8/25] Train Loss: 0.6955, Train Acc: 0.8839, Val Loss: 1.1721, Val Acc: 0.7264\nEpoch [9/25] Train Loss: 0.6188, Train Acc: 0.8844, Val Loss: 1.1650, Val Acc: 0.7432\nEpoch [10/25] Train Loss: 0.6316, Train Acc: 0.8947, Val Loss: 1.1396, Val Acc: 0.7095\nEpoch [12/25] Train Loss: 0.6528, Train Acc: 0.8956, Val Loss: 1.0947, Val Acc: 0.7466\nEpoch [13/25] Train Loss: 0.6630, Train Acc: 0.8876, Val Loss: 1.1065, Val Acc: 0.7466\nEpoch [14/25] Train Loss: 0.5938, Train Acc: 0.9018, Val Loss: 1.1299, Val Acc: 0.7466\nEpoch [15/25] Train Loss: 0.5663, Train Acc: 0.9104, Val Loss: 1.0779, Val Acc: 0.7568\nEpoch [16/25] Train Loss: 0.6334, Train Acc: 0.8935, Val Loss: 1.1907, Val Acc: 0.7568\nEpoch [17/25] Train Loss: 0.6871, Train Acc: 0.8897, Val Loss: 1.2110, Val Acc: 0.7264\nEpoch [18/25] Train Loss: 0.5605, Train Acc: 0.9071, Val Loss: 1.1681, Val Acc: 0.7399\nEpoch [19/25] Train Loss: 0.6242, Train Acc: 0.8902, Val Loss: 1.1168, Val Acc: 0.7365\nEpoch 00019: reducing learning rate of group 0 to 3.0000e-06.\nEpoch [20/25] Train Loss: 0.5292, Train Acc: 0.9057, Val Loss: 1.1357, Val Acc: 0.7432\nEpoch [21/25] Train Loss: 0.5785, Train Acc: 0.9010, Val Loss: 1.0985, Val Acc: 0.7399\nEpoch [22/25] Train Loss: 0.6580, Train Acc: 0.8775, Val Loss: 1.1264, Val Acc: 0.7365\nEpoch [23/25] Train Loss: 0.6146, Train Acc: 0.8882, Val Loss: 1.1115, Val Acc: 0.7297\nEpoch 00023: reducing learning rate of group 0 to 3.0000e-07.\nEpoch [24/25] Train Loss: 0.6047, Train Acc: 0.8934, Val Loss: 1.1279, Val Acc: 0.7432\nEpoch [25/25] Train Loss: 0.5877, Train Acc: 0.9057, Val Loss: 1.1337, Val Acc: 0.7264\nTraining with lr=0.0003, weight_decay=0.0001\nEpoch [1/25] Train Loss: 2.1219, Train Acc: 0.4595, Val Loss: 1.8090, Val Acc: 0.4662\nEpoch [2/25] Train Loss: 1.1373, Train Acc: 0.7147, Val Loss: 1.5488, Val Acc: 0.5912\nEpoch [3/25] Train Loss: 1.1036, Train Acc: 0.7441, Val Loss: 1.7681, Val Acc: 0.5980\nEpoch [4/25] Train Loss: 0.9264, Train Acc: 0.7989, Val Loss: 1.7226, Val Acc: 0.5946\nEpoch [5/25] Train Loss: 0.8650, Train Acc: 0.8270, Val Loss: 1.6502, Val Acc: 0.6115\nEpoch [6/25] Train Loss: 0.8112, Train Acc: 0.8452, Val Loss: 1.4055, Val Acc: 0.6723\nEpoch [7/25] Train Loss: 0.7167, Train Acc: 0.8831, Val Loss: 1.1823, Val Acc: 0.7534\nEpoch [8/25] Train Loss: 0.6649, Train Acc: 0.8903, Val Loss: 1.3307, Val Acc: 0.7061\nEpoch [9/25] Train Loss: 0.6493, Train Acc: 0.8891, Val Loss: 1.1469, Val Acc: 0.7568\nEpoch [10/25] Train Loss: 0.5516, Train Acc: 0.9060, Val Loss: 1.1210, Val Acc: 0.7635\nEpoch [11/25] Train Loss: 0.6405, Train Acc: 0.8967, Val Loss: 1.1210, Val Acc: 0.7534\nEpoch [12/25] Train Loss: 0.6407, Train Acc: 0.8994, Val Loss: 1.1985, Val Acc: 0.7466\nEpoch [13/25] Train Loss: 0.7425, Train Acc: 0.8684, Val Loss: 1.1855, Val Acc: 0.7466\nEpoch [14/25] Train Loss: 0.6326, Train Acc: 0.8906, Val Loss: 1.1779, Val Acc: 0.7331\nEpoch [15/25] Train Loss: 0.5960, Train Acc: 0.9007, Val Loss: 1.1953, Val Acc: 0.7432\nEpoch [16/25] Train Loss: 0.6583, Train Acc: 0.8882, Val Loss: 1.2479, Val Acc: 0.7128\nEpoch [17/25] Train Loss: 0.6985, Train Acc: 0.8732, Val Loss: 1.3719, Val Acc: 0.7162\nEpoch [18/25] Train Loss: 0.6402, Train Acc: 0.8874, Val Loss: 1.4525, Val Acc: 0.6655\nEpoch [19/25] Train Loss: 0.6721, Train Acc: 0.8724, Val Loss: 1.5157, Val Acc: 0.6622\nEpoch [20/25] Train Loss: 0.8003, Train Acc: 0.8431, Val Loss: 2.2214, Val Acc: 0.5777\nEpoch [21/25] Train Loss: 0.8441, Train Acc: 0.8417, Val Loss: 1.4691, Val Acc: 0.6216\nEpoch [22/25] Train Loss: 0.6976, Train Acc: 0.8702, Val Loss: 1.4801, Val Acc: 0.6486\nEpoch [23/25] Train Loss: 0.7571, Train Acc: 0.8513, Val Loss: 1.5597, Val Acc: 0.6520\nEpoch [24/25] Train Loss: 0.6216, Train Acc: 0.8924, Val Loss: 1.8091, Val Acc: 0.6385\nEpoch [25/25] Train Loss: 0.7835, Train Acc: 0.8538, Val Loss: 1.2402, Val Acc: 0.7128\nEarly stopping triggered\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 184\u001b[0m\n\u001b[1;32m    172\u001b[0m             results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    173\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: lr,\n\u001b[1;32m    174\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: wd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracies\u001b[39m\u001b[38;5;124m'\u001b[39m: val_accuracies\n\u001b[1;32m    181\u001b[0m             })\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mace_tools\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m; tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel Training Results\u001b[39m\u001b[38;5;124m\"\u001b[39m, pd\u001b[38;5;241m.\u001b[39mDataFrame(results))\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
          ],
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ace_tools'",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-wXvMPjVZxJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GJ5cpTcUZxGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UiL5fq4eZxDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdZ8eZ8bZwlH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}