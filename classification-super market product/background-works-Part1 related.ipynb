{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 6283815,
          "sourceType": "datasetVersion",
          "datasetId": 3613373
        }
      ],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when create a version using \"Save & Run All\"\n",
        "# can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "5zQSTNbJTfOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import\n",
        "#imports\n",
        "from torch import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from itertools import cycle\n",
        "import optuna\n",
        "import optuna\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torch import nn, optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import Sampler\n",
        "import random"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T15:08:52.711371Z",
          "iopub.execute_input": "2024-07-02T15:08:52.711742Z",
          "iopub.status.idle": "2024-07-02T15:08:52.751390Z",
          "shell.execute_reply.started": "2024-07-02T15:08:52.711718Z",
          "shell.execute_reply": "2024-07-02T15:08:52.750235Z"
        },
        "trusted": true,
        "id": "GIcr6SKZTfOG",
        "outputId": "c60f225e-8a98-4826-fb30-8127bc001d9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#import\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#imports\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReduceLROnPlateau\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlr_scheduler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReduceLROnPlateau\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'AdamW' from 'torch' (/opt/conda/lib/python3.10/site-packages/torch/__init__.py)"
          ],
          "ename": "ImportError",
          "evalue": "cannot import name 'AdamW' from 'torch' (/opt/conda/lib/python3.10/site-packages/torch/__init__.py)",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "import glob\n",
        "\n",
        "class ImageNetDataset(Dataset):\n",
        "    def __init__(self, dataset_dir: str, transform=None) -> None:\n",
        "        self.transform = transform\n",
        "        self.samples = []\n",
        "        classes = [d.name for d in os.scandir(dataset_dir) if d.is_dir()]\n",
        "\n",
        "        for label, class_name in enumerate(classes):\n",
        "            class_dir = Path(dataset_dir) / class_name\n",
        "            class_files = glob.glob(str(class_dir / '*.jpg'))  # or other image extension\n",
        "            self.samples += [(f, label) for f in class_files]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[torch.Tensor, int]:\n",
        "        img_path, label = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Define the dataset directory\n",
        "dataset_dir = \"/kaggle/input/imagenet-256\"\n",
        "\n",
        "# Data transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Keeps the size transformation\n",
        "        transforms.RandomHorizontalFlip(),  # Retain random horizontal flip\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalization\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "# Load the full dataset\n",
        "full_dataset = ImageNetDataset(dataset_dir, transform=data_transforms['train'])\n",
        "\n",
        "\n",
        "# Define the split ratios\n",
        "train_ratio, val_ratio, test_ratio = 0.08, 0.01, 0.01\n",
        "\n",
        "# Calculate lengths for each subset\n",
        "total_len = len(full_dataset)\n",
        "train_len = int(train_ratio * total_len)\n",
        "val_len = int(val_ratio * total_len)\n",
        "test_len = total_len - train_len - val_len\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_len, val_len, test_len])\n",
        "\n",
        "# Apply different transformations to each subset\n",
        "train_dataset.dataset.transform = data_transforms['train']\n",
        "val_dataset.dataset.transform = data_transforms['val']\n",
        "test_dataset.dataset.transform = data_transforms['test']\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader_imgnt = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader_imgnt = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "test_loader_imgnt = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T12:35:04.562092Z",
          "iopub.execute_input": "2024-07-02T12:35:04.563021Z",
          "iopub.status.idle": "2024-07-02T12:39:00.975727Z",
          "shell.execute_reply.started": "2024-07-02T12:35:04.562985Z",
          "shell.execute_reply": "2024-07-02T12:39:00.974029Z"
        },
        "trusted": true,
        "id": "pjL3BvYnTfOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T12:41:46.060975Z",
          "iopub.execute_input": "2024-07-02T12:41:46.061621Z",
          "iopub.status.idle": "2024-07-02T12:41:46.066134Z",
          "shell.execute_reply.started": "2024-07-02T12:41:46.061590Z",
          "shell.execute_reply": "2024-07-02T12:41:46.065259Z"
        },
        "trusted": true,
        "id": "XGVbQhdfTfOH",
        "outputId": "2673722b-b64a-434c-ee0d-e1cd55f8401f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "30703\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super(DenseLayer, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(self.relu(self.bn1(x)))\n",
        "        out = torch.cat([x, out], 1)\n",
        "        return out\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, n_layers):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(n_layers):\n",
        "            layers.append(DenseLayer(in_channels + i * growth_rate, growth_rate))\n",
        "        self.layer = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class TransitionLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(TransitionLayer, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(self.relu(self.bn(x)))\n",
        "        out = self.pool(out)\n",
        "        return out\n",
        "\n",
        "class CustomDenseNet(nn.Module):\n",
        "    def __init__(self, num_classes=43, growth_rate=32, block_layers=[6, 12, 24, 16]):\n",
        "        super(CustomDenseNet, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "        num_blocks = len(block_layers)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Dense blocks and transition layers\n",
        "        in_channels = 64\n",
        "        self.dense_blocks = nn.ModuleList()\n",
        "        self.trans_layers = nn.ModuleList()\n",
        "        for i in range(num_blocks):\n",
        "            self.dense_blocks.append(DenseBlock(in_channels, growth_rate, block_layers[i]))\n",
        "            in_channels += growth_rate * block_layers[i]\n",
        "            if i != num_blocks - 1:  # No transition layer after the last dense block\n",
        "                out_channels = in_channels // 2\n",
        "                self.trans_layers.append(TransitionLayer(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "\n",
        "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
        "        self.fc = nn.Linear(in_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu(self.bn1(self.conv1(x))))\n",
        "        for i in range(len(self.dense_blocks)):\n",
        "            x = self.dense_blocks[i](x)\n",
        "            if i != len(self.dense_blocks) - 1:\n",
        "                x = self.trans_layers[i](x)\n",
        "        x = self.relu(self.bn2(x))\n",
        "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=15, verbose=False, delta=0):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.best_model_wts = None\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decreases.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        self.best_model_wts = model.state_dict()\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "    def get_best_model_wts(self):\n",
        "        return self.best_model_wts\n",
        "\n",
        "def train_model_customCNN(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=25, patience=15):\n",
        "    best_val_acc = 0.0\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = val_correct / val_total\n",
        "        val_losses.append(val_epoch_loss)\n",
        "        val_accuracies.append(val_epoch_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
        "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "        early_stopping(val_epoch_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping triggered\")\n",
        "            model.load_state_dict(early_stopping.get_best_model_wts())\n",
        "            break\n",
        "\n",
        "        if val_epoch_acc > best_val_acc:\n",
        "            best_val_acc = val_epoch_acc\n",
        "            best_model_wts = model.state_dict()\n",
        "\n",
        "        if isinstance(scheduler, ReduceLROnPlateau):\n",
        "            scheduler.step(val_epoch_loss)  # Pass validation loss to scheduler\n",
        "        else:\n",
        "            scheduler.step()\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    misclassifications = defaultdict(list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            # Collect misclassified examples\n",
        "            for label, prediction in zip(labels, predicted):\n",
        "                if label != prediction:\n",
        "                    misclassifications[label.item()].append(prediction.item())\n",
        "\n",
        "    acc = correct / total\n",
        "    print(f'Test Accuracy: {acc:.4f}')\n",
        "\n",
        "    # Print misclassification details\n",
        "    for label, predictions in misclassifications.items():\n",
        "        print(f'{len(predictions)} data from class number {label} wrongly labeled as {predictions}')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T17:40:55.582604Z",
          "iopub.execute_input": "2024-07-01T17:40:55.582946Z",
          "iopub.status.idle": "2024-07-01T17:40:55.622723Z",
          "shell.execute_reply.started": "2024-07-01T17:40:55.582921Z",
          "shell.execute_reply": "2024-07-01T17:40:55.621802Z"
        },
        "trusted": true,
        "id": "-TiuU42uTfOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training function\n",
        "def train_model_imagenet(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs=20):\n",
        "    best_val_acc = 0.0\n",
        "    train_losses, val_losses = [], []\n",
        "    train_accuracies, val_accuracies = [], []\n",
        "    early_stopping = EarlyStopping(patience=15)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_acc = correct / total\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_accuracies.append(epoch_acc)\n",
        "\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_running_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n",
        "        val_epoch_acc = val_correct / val_total\n",
        "        val_losses.append(val_epoch_loss)\n",
        "        val_accuracies.append(val_epoch_acc)\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, '\n",
        "              f'Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.4f}')\n",
        "\n",
        "        scheduler.step(val_epoch_acc)\n",
        "\n",
        "        if val_epoch_acc > best_val_acc:\n",
        "            best_val_acc = val_epoch_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            torch.save(optimizer.state_dict(), 'best_opt.pth')\n",
        "\n",
        "    print(f'Best Validation Accuracy: {best_val_acc:.4f}')\n",
        "    return model, train_losses, val_losses, train_accuracies, val_accuracies\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T15:12:31.171404Z",
          "iopub.execute_input": "2024-07-02T15:12:31.172102Z",
          "iopub.status.idle": "2024-07-02T15:12:31.185777Z",
          "shell.execute_reply.started": "2024-07-02T15:12:31.172074Z",
          "shell.execute_reply": "2024-07-02T15:12:31.184839Z"
        },
        "trusted": true,
        "id": "X5Qzn5D0TfOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# Assuming the data loading and dataset classes are defined as per provided code\n",
        "\n",
        "# Adjust these parameters as needed\n",
        "epochs = 20\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize DenseNet model\n",
        "num_classes = len(class_names)  # Ensure class_names is defined properly\n",
        "model = CustomDenseNet(num_classes=num_classes, growth_rate=32).to(device)\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "\n",
        "# Assuming train_loader and val_loader are already defined from previous data loading steps\n",
        "\n",
        "# Train the model\n",
        "model, train_losses, val_losses, train_accuracies, val_accuracies = train_model(model, nn.CrossEntropyLoss(), optimizer, scheduler, train_loader, val_loader, num_epochs=epochs)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T17:45:36.338181Z",
          "iopub.execute_input": "2024-07-01T17:45:36.339079Z",
          "iopub.status.idle": "2024-07-01T19:07:57.515885Z",
          "shell.execute_reply.started": "2024-07-01T17:45:36.339046Z",
          "shell.execute_reply": "2024-07-01T19:07:57.514707Z"
        },
        "id": "GRKO5sMZTfOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 40\n",
        "model_copy = CustomDenseNet(1000)  # Replace with model class\n",
        "\n",
        "# Load the state_dict into the new model instance\n",
        "model_copy.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "# Load the optimizer state_dict\n",
        "#checkpoint = torch.load('/kaggle/working/best_model.pth')\n",
        "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "model_copy.to(device)\n",
        "new_model, train_losses, val_losses, train_accuracies, val_accuracies = train_model_imagenet(model_copy, nn.CrossEntropyLoss(), optimizer, scheduler, test_loader_imgnt, val_loader_imgnt, num_epochs=epochs)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T15:12:40.257916Z",
          "iopub.execute_input": "2024-07-02T15:12:40.258786Z",
          "iopub.status.idle": "2024-07-02T17:37:51.321875Z",
          "shell.execute_reply.started": "2024-07-02T15:12:40.258748Z",
          "shell.execute_reply": "2024-07-02T17:37:51.320258Z"
        },
        "trusted": true,
        "id": "PlJojmDETfOJ",
        "outputId": "b65f46b2-248e-4a4f-c73c-3e8281c1b7fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/40] Train Loss: 4.3267, Train Acc: 0.2360, Val Loss: 4.1243, Val Acc: 0.2679\nEpoch [2/40] Train Loss: 4.3267, Train Acc: 0.2360, Val Loss: 4.1243, Val Acc: 0.2679\nEpoch [3/40] Train Loss: 4.3267, Train Acc: 0.2360, Val Loss: 4.1243, Val Acc: 0.2679\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the optimizer state_dict\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#checkpoint = torch.load('/kaggle/working/best_model.pth')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m model_copy\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m new_model, train_losses, val_losses, train_accuracies, val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_imagenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader_imgnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader_imgnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mtrain_model_imagenet\u001b[0;34m(model, criterion, optimizer, scheduler, train_loader, val_loader, num_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ],
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), '/kaggle/working/model_weights.h')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T19:20:06.905475Z",
          "iopub.execute_input": "2024-07-01T19:20:06.905844Z",
          "iopub.status.idle": "2024-07-01T19:20:07.006468Z",
          "shell.execute_reply.started": "2024-07-01T19:20:06.905815Z",
          "shell.execute_reply": "2024-07-01T19:20:07.005491Z"
        },
        "trusted": true,
        "id": "76tsjw9RTfOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "\n",
        "FileLink('/kaggle/working/best_model.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T19:22:31.116895Z",
          "iopub.execute_input": "2024-07-01T19:22:31.117511Z",
          "iopub.status.idle": "2024-07-01T19:22:31.123574Z",
          "shell.execute_reply.started": "2024-07-01T19:22:31.117477Z",
          "shell.execute_reply": "2024-07-01T19:22:31.122719Z"
        },
        "trusted": true,
        "id": "tl7FBwnjTfOJ",
        "outputId": "fa81df8b-836e-4c68-8469-b1ac5949eeb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 30,
          "output_type": "execute_result",
          "data": {
            "text/plain": "/kaggle/working/best_model.pth",
            "text/html": "<a href='/kaggle/working/best_model.pth' target='_blank'>/kaggle/working/best_model.pth</a><br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(r'/kaggle/working')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T19:22:12.788832Z",
          "iopub.execute_input": "2024-07-01T19:22:12.789522Z",
          "iopub.status.idle": "2024-07-01T19:22:12.793480Z",
          "shell.execute_reply.started": "2024-07-01T19:22:12.789489Z",
          "shell.execute_reply": "2024-07-01T19:22:12.792508Z"
        },
        "trusted": true,
        "id": "kqwyqRiCTfOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Define the file paths\n",
        "file_to_zip = '/kaggle/working/best_model.pth'\n",
        "output_zip = '/kaggle/working/model.zip'\n",
        "\n",
        "# Zip the file\n",
        "shutil.make_archive(output_zip.replace('.zip', ''), 'zip', '/kaggle/working', 'best_model.pth')\n",
        "\n",
        "print(\"File zipped successfully!\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T19:25:23.913052Z",
          "iopub.execute_input": "2024-07-01T19:25:23.913899Z",
          "iopub.status.idle": "2024-07-01T19:25:26.324292Z",
          "shell.execute_reply.started": "2024-07-01T19:25:23.913837Z",
          "shell.execute_reply": "2024-07-01T19:25:26.323337Z"
        },
        "trusted": true,
        "id": "fCLr1pFpTfOK",
        "outputId": "2dec6ab7-846b-4272-e052-3ab3ce1f2d65"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "File zipped successfully!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink\n",
        "\n",
        "FileLink('/kaggle/working/model.zip')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T19:30:11.732600Z",
          "iopub.execute_input": "2024-07-01T19:30:11.732999Z",
          "iopub.status.idle": "2024-07-01T19:30:11.739403Z",
          "shell.execute_reply.started": "2024-07-01T19:30:11.732969Z",
          "shell.execute_reply": "2024-07-01T19:30:11.738562Z"
        },
        "trusted": true,
        "id": "d83mbnTkTfOK",
        "outputId": "9b907169-de45-483e-a0c4-46dc024aa71b"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 32,
          "output_type": "execute_result",
          "data": {
            "text/plain": "/kaggle/working/model.zip",
            "text/html": "<a href='/kaggle/working/model.zip' target='_blank'>/kaggle/working/model.zip</a><br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import FileLink, display\n",
        "display(FileLink('/kaggle/working/model.zip'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T19:37:16.068010Z",
          "iopub.execute_input": "2024-07-01T19:37:16.068784Z",
          "iopub.status.idle": "2024-07-01T19:37:16.074649Z",
          "shell.execute_reply.started": "2024-07-01T19:37:16.068752Z",
          "shell.execute_reply": "2024-07-01T19:37:16.073774Z"
        },
        "trusted": true,
        "id": "ORzd2u45TfOK",
        "outputId": "f0d65369-bfea-4d47-91cc-a14f83593b11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "/kaggle/working/model.zip",
            "text/html": "<a href='/kaggle/working/model.zip' target='_blank'>/kaggle/working/model.zip</a><br>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZh8YoZLTfOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ly-IhrSlTfOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/marcusklasson/GroceryStoreDataset.git"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T19:41:12.649953Z",
          "iopub.execute_input": "2024-07-01T19:41:12.650416Z",
          "iopub.status.idle": "2024-07-01T19:41:23.743357Z",
          "shell.execute_reply.started": "2024-07-01T19:41:12.650387Z",
          "shell.execute_reply": "2024-07-01T19:41:23.742370Z"
        },
        "trusted": true,
        "id": "MLZcGhCSTfOK",
        "outputId": "f6d5ab0f-e593-4bbe-d219-eace7ed25f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Cloning into 'GroceryStoreDataset'...\nremote: Enumerating objects: 6559, done.\u001b[K\nremote: Counting objects: 100% (266/266), done.\u001b[K\nremote: Compressing objects: 100% (231/231), done.\u001b[K\nremote: Total 6559 (delta 45), reused 35 (delta 35), pack-reused 6293\u001b[K\nReceiving objects: 100% (6559/6559), 116.26 MiB | 15.17 MiB/s, done.\nResolving deltas: 100% (275/275), done.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from PIL import Image\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Tuple\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "num_classes = 43\n",
        "\n",
        "class GroceryStoreDataset(Dataset):\n",
        "    def __init__(self, samples: List[Tuple[str, int]] = None, split: str = None, transform=None) -> None:\n",
        "        super().__init__()\n",
        "        self.root = Path(\"/kaggle/working/GroceryStoreDataset/dataset\")\n",
        "        self.transform = transform\n",
        "\n",
        "        if samples is not None:\n",
        "            self.samples = samples\n",
        "            self.labels = [label for _, label in samples]\n",
        "            self.paths = [path for path, _ in samples]\n",
        "        else:\n",
        "            self.split = split\n",
        "            self.paths, self.labels = self.read_file()\n",
        "\n",
        "        self.class_names = self.get_class_names()\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
        "        img = Image.open(self.root / self.paths[idx]).convert(\"RGB\")\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "    def read_file(self) -> Tuple[List[str], List[int]]:\n",
        "        paths = []\n",
        "        labels = []\n",
        "        with open(self.root / f\"{self.split}.txt\") as f:\n",
        "            for line in f:\n",
        "                path, _, label = line.strip().split(\", \")\n",
        "                paths.append(path)\n",
        "                labels.append(int(label))\n",
        "        return paths, labels\n",
        "\n",
        "    def get_class_names(self) -> List[str]:\n",
        "        class_names = {}\n",
        "        with open(self.root / \"train.txt\") as f:\n",
        "            for line in f:\n",
        "                path, _, label = line.strip().split(\", \")\n",
        "                if int(label) not in class_names:\n",
        "                    class_names[int(label)] = Path(path).parent.name\n",
        "        return [class_names[i] for i in range(len(class_names))]\n",
        "\n",
        "    def get_num_classes(self) -> int:\n",
        "        return max(self.labels) + 1\n",
        "\n",
        "# Sample dataset loader\n",
        "def sample_max_50(samples, dynamic_size=50):\n",
        "    sampled_samples = []\n",
        "    class_count = Counter([label for _, label in samples])\n",
        "    class_samples = {k: [] for k in class_count.keys()}\n",
        "\n",
        "    for img_path, label in samples:\n",
        "        class_samples[label].append((img_path, label))\n",
        "\n",
        "    for cls in class_samples.keys():\n",
        "        sampled_samples.extend(random.sample(class_samples[cls], min(dynamic_size, class_count[cls])))\n",
        "\n",
        "    return sampled_samples\n",
        "\n",
        "def filter_samples(samples, target_classes):\n",
        "    return [(img_path, label) for img_path, label in samples if label in target_classes]\n",
        "\n",
        "# Classes with high and low number of instances (as an example)\n",
        "#high_instance_classes = [0, 7, 13, 19, 20, 27, 38, 39, 41]\n",
        "#low_instance_classes = [10, 15, 24, 28, 30, 34, 40]\n",
        "high_instance_classes = [0, 19, 20, 27, 7, 41, 13, 38, 39]\n",
        "low_instance_classes = [24, 42, 11, 33, 14, 16, 34, 36, 15, 29, 10, 31, 35, 40, 28]\n",
        "l3 = high_instance_classes+low_instance_classes\n",
        "rest_instances = list(range(0,43))\n",
        "for i in l3:\n",
        "  rest_instances.remove(i)\n",
        "\n",
        "\n",
        "# Paths to dataset splits\n",
        "path2train = \"/kaggle/working/GroceryStoreDataset/dataset/train\"\n",
        "path2val = \"/kaggle/working/GroceryStoreDataset/dataset/val\"\n",
        "path2test = \"/kaggle/working/GroceryStoreDataset/dataset/test\"\n",
        "\n",
        "# Data transformations\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "\n",
        "# Load the datasets\n",
        "train_dataset_full = GroceryStoreDataset(split='train', transform=data_transforms['train'])\n",
        "val_dataset_full = GroceryStoreDataset(split='val', transform=data_transforms['val'])\n",
        "test_dataset_full = GroceryStoreDataset(split='test', transform=data_transforms['test'])\n",
        "\n",
        "# Prepare samples\n",
        "train_samples = list(zip(train_dataset_full.paths, train_dataset_full.labels))\n",
        "val_samples = list(zip(val_dataset_full.paths, val_dataset_full.labels))\n",
        "\n",
        "train_samples_50 = sample_max_50(train_samples, 36)\n",
        "train_samples_high = filter_samples(train_samples, high_instance_classes)\n",
        "train_samples_low = filter_samples(train_samples, low_instance_classes)\n",
        "\n",
        "# Create datasets using the filtered samples\n",
        "train_dataset_50 = GroceryStoreDataset(samples=train_samples_50, transform=data_transforms['train'])\n",
        "train_dataset_high = GroceryStoreDataset(samples=train_samples_high, transform=data_transforms['train'])\n",
        "train_dataset_low = GroceryStoreDataset(samples=train_samples_low, transform=data_transforms['train'])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader_50 = DataLoader(train_dataset_50, batch_size=32, shuffle=True, num_workers=4)\n",
        "train_loader_high = DataLoader(train_dataset_high, batch_size=32, shuffle=True, num_workers=4)\n",
        "train_loader_low = DataLoader(train_dataset_low, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "# Prepare validation samples\n",
        "val_samples_50 = sample_max_50(val_samples)\n",
        "val_samples_high = filter_samples(val_samples, high_instance_classes)\n",
        "val_samples_low = filter_samples(val_samples, low_instance_classes)\n",
        "\n",
        "# Create validation datasets using the filtered samples\n",
        "val_dataset_50 = GroceryStoreDataset(samples=val_samples_50, transform=data_transforms['val'])\n",
        "val_dataset_high = GroceryStoreDataset(samples=val_samples_high, transform=data_transforms['val'])\n",
        "val_dataset_low = GroceryStoreDataset(samples=val_samples_low, transform=data_transforms['val'])\n",
        "\n",
        "# Create validation DataLoaders\n",
        "val_loader_50 = DataLoader(val_dataset_50, batch_size=32, shuffle=False, num_workers=4)\n",
        "val_loader_high = DataLoader(val_dataset_high, batch_size=32, shuffle=False, num_workers=4)\n",
        "val_loader_low = DataLoader(val_dataset_low, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "# Now can proceed with training and evaluation as before\n",
        "train_dataset = GroceryStoreDataset(split='train', transform=data_transforms['train'])\n",
        "val_dataset = GroceryStoreDataset(split='val', transform=data_transforms['val'])\n",
        "test_dataset = GroceryStoreDataset(split='test', transform=data_transforms['test'])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T19:41:44.446543Z",
          "iopub.execute_input": "2024-07-01T19:41:44.446915Z",
          "iopub.status.idle": "2024-07-01T19:41:44.546050Z",
          "shell.execute_reply.started": "2024-07-01T19:41:44.446884Z",
          "shell.execute_reply": "2024-07-01T19:41:44.545050Z"
        },
        "trusted": true,
        "id": "9RkDdeW7TfOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l3 = high_instance_classes+low_instance_classes\n",
        "rest_instances = list(range(0,43))\n",
        "for i in l3:\n",
        "  rest_instances.remove(i)\n",
        "\n",
        "alternative_data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "}\n",
        "\n",
        "train_samples_rest = filter_samples(train_samples, rest_instances)\n",
        "train_dataset_rest = GroceryStoreDataset(samples=train_samples_rest, transform=alternative_data_transforms['train'])\n",
        "train_loader_rest = DataLoader(train_dataset_rest, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_samples_rest = filter_samples(val_samples, rest_instances)\n",
        "val_dataset_rest = GroceryStoreDataset(samples=val_samples_rest, transform=alternative_data_transforms['val'])\n",
        "val_loader_rest = DataLoader(val_dataset_rest, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T20:10:21.789591Z",
          "iopub.execute_input": "2024-07-01T20:10:21.790086Z",
          "iopub.status.idle": "2024-07-01T20:10:21.813814Z",
          "shell.execute_reply.started": "2024-07-01T20:10:21.790048Z",
          "shell.execute_reply": "2024-07-01T20:10:21.812920Z"
        },
        "trusted": true,
        "id": "1KDjgSKvTfOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SKXmCOPFTfOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_copy = CustomDenseNet(1000)  # Replace with model class\n",
        "\n",
        "# Load the state_dict into the new model instance\n",
        "model_copy.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:37:54.578662Z",
          "iopub.execute_input": "2024-07-02T17:37:54.579020Z",
          "iopub.status.idle": "2024-07-02T17:37:54.807878Z",
          "shell.execute_reply.started": "2024-07-02T17:37:54.578990Z",
          "shell.execute_reply": "2024-07-02T17:37:54.806968Z"
        },
        "trusted": true,
        "id": "8AmgYy8wTfOL",
        "outputId": "7f7b9e11-dad8-468a-e9fc-652dd57f6823"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 21,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = CustomDenseNet(1000)  # Replace with model class\n",
        "\n",
        "# Load the state_dict into the new model instance\n",
        "model2.load_state_dict(torch.load('/kaggle/working/best_model.pth'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:38:02.418336Z",
          "iopub.execute_input": "2024-07-02T17:38:02.418951Z",
          "iopub.status.idle": "2024-07-02T17:38:02.643487Z",
          "shell.execute_reply.started": "2024-07-02T17:38:02.418922Z",
          "shell.execute_reply": "2024-07-02T17:38:02.642538Z"
        },
        "trusted": true,
        "id": "QxnEOSkbTfOL",
        "outputId": "4af37295-13c4-4936-d90d-55dc20f11d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 22,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = CustomDenseNet(1000)  # Replace with model class\n",
        "\n",
        "# Load the state_dict into the new model instance\n",
        "model3.load_state_dict(torch.load('/kaggle/working/best_model.pth'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:38:04.462557Z",
          "iopub.execute_input": "2024-07-02T17:38:04.462920Z",
          "iopub.status.idle": "2024-07-02T17:38:04.692776Z",
          "shell.execute_reply.started": "2024-07-02T17:38:04.462890Z",
          "shell.execute_reply": "2024-07-02T17:38:04.691879Z"
        },
        "trusted": true,
        "id": "r5nEBoTNTfOL",
        "outputId": "f968650b-31ba-445c-de6e-5d5de6eca4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_ftrs = model_copy.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 43)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:38:07.433943Z",
          "iopub.execute_input": "2024-07-02T17:38:07.434780Z",
          "iopub.status.idle": "2024-07-02T17:38:07.439621Z",
          "shell.execute_reply.started": "2024-07-02T17:38:07.434744Z",
          "shell.execute_reply": "2024-07-02T17:38:07.438681Z"
        },
        "trusted": true,
        "id": "gq4MyEYpTfOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.2\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(num_ftrs, 43)\n",
        "            )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:38:09.212651Z",
          "iopub.execute_input": "2024-07-02T17:38:09.213354Z",
          "iopub.status.idle": "2024-07-02T17:38:09.218445Z",
          "shell.execute_reply.started": "2024-07-02T17:38:09.213323Z",
          "shell.execute_reply": "2024-07-02T17:38:09.217522Z"
        },
        "trusted": true,
        "id": "Q0y37oogTfOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sk--ExQATfOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fbpTO0tDTfOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:38:20.403627Z",
          "iopub.execute_input": "2024-07-02T17:38:20.403995Z",
          "iopub.status.idle": "2024-07-02T17:38:20.412496Z",
          "shell.execute_reply.started": "2024-07-02T17:38:20.403966Z",
          "shell.execute_reply": "2024-07-02T17:38:20.411573Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true,
        "id": "swzgQQaETfOM",
        "outputId": "357caf3c-873a-4cf9-f9ee-d182b14bc5ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "CustomDenseNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (dense_blocks): ModuleList(\n    (0): DenseBlock(\n      (layer): Sequential(\n        (0): DenseLayer(\n          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (1): DenseLayer(\n          (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (2): DenseLayer(\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (3): DenseLayer(\n          (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (4): DenseLayer(\n          (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (5): DenseLayer(\n          (bn1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(224, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n    )\n    (1): DenseBlock(\n      (layer): Sequential(\n        (0): DenseLayer(\n          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (1): DenseLayer(\n          (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(160, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (2): DenseLayer(\n          (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(192, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (3): DenseLayer(\n          (bn1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(224, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (4): DenseLayer(\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (5): DenseLayer(\n          (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(288, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (6): DenseLayer(\n          (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (7): DenseLayer(\n          (bn1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(352, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (8): DenseLayer(\n          (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(384, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (9): DenseLayer(\n          (bn1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(416, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (10): DenseLayer(\n          (bn1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(448, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (11): DenseLayer(\n          (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(480, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n    )\n    (2): DenseBlock(\n      (layer): Sequential(\n        (0): DenseLayer(\n          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (1): DenseLayer(\n          (bn1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(288, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (2): DenseLayer(\n          (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (3): DenseLayer(\n          (bn1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(352, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (4): DenseLayer(\n          (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(384, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (5): DenseLayer(\n          (bn1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(416, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (6): DenseLayer(\n          (bn1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(448, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (7): DenseLayer(\n          (bn1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(480, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (8): DenseLayer(\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (9): DenseLayer(\n          (bn1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(544, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (10): DenseLayer(\n          (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(576, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (11): DenseLayer(\n          (bn1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(608, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (12): DenseLayer(\n          (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(640, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (13): DenseLayer(\n          (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(672, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (14): DenseLayer(\n          (bn1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(704, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (15): DenseLayer(\n          (bn1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(736, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (16): DenseLayer(\n          (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(768, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (17): DenseLayer(\n          (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(800, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (18): DenseLayer(\n          (bn1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(832, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (19): DenseLayer(\n          (bn1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(864, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (20): DenseLayer(\n          (bn1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(896, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (21): DenseLayer(\n          (bn1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(928, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (22): DenseLayer(\n          (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(960, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (23): DenseLayer(\n          (bn1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(992, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n    )\n    (3): DenseBlock(\n      (layer): Sequential(\n        (0): DenseLayer(\n          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(512, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (1): DenseLayer(\n          (bn1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(544, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (2): DenseLayer(\n          (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(576, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (3): DenseLayer(\n          (bn1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(608, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (4): DenseLayer(\n          (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(640, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (5): DenseLayer(\n          (bn1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(672, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (6): DenseLayer(\n          (bn1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(704, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (7): DenseLayer(\n          (bn1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(736, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (8): DenseLayer(\n          (bn1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(768, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (9): DenseLayer(\n          (bn1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(800, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (10): DenseLayer(\n          (bn1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(832, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (11): DenseLayer(\n          (bn1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(864, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (12): DenseLayer(\n          (bn1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(896, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (13): DenseLayer(\n          (bn1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(928, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (14): DenseLayer(\n          (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(960, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (15): DenseLayer(\n          (bn1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (relu): ReLU(inplace=True)\n          (conv1): Conv2d(992, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n  (trans_layers): ModuleList(\n    (0): TransitionLayer(\n      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (1): TransitionLayer(\n      (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n    (2): TransitionLayer(\n      (bn): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n    )\n  )\n  (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc): Sequential(\n    (0): Dropout(p=0.2, inplace=False)\n    (1): Linear(in_features=1024, out_features=43, bias=True)\n  )\n)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 35\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_low_dens = model#CustomDenseNet(num_classes)\n",
        "model_low_dens = model_low_dens.to(device)\n",
        "optimizer_low = optim.AdamW(model_low_dens.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler_low = ReduceLROnPlateau(optimizer_low, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "#model_high = train_individual_model(model_high, train_loader_high, nn.CrossEntropyLoss(), optimizer_high, scheduler_high)\n",
        "model_low_dens = train_model_customCNN(model_low_dens, nn.CrossEntropyLoss(), optimizer_low, scheduler_low, train_loader_low, val_loader_low, num_epochs=epochs)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T20:02:03.126055Z",
          "iopub.execute_input": "2024-07-01T20:02:03.126760Z",
          "iopub.status.idle": "2024-07-01T20:03:20.151382Z",
          "shell.execute_reply.started": "2024-07-01T20:02:03.126724Z",
          "shell.execute_reply": "2024-07-01T20:03:20.150235Z"
        },
        "trusted": true,
        "id": "frPe6NijTfOM",
        "outputId": "3994638b-7068-4825-f379-4d36dcf1eaad"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/35] Train Loss: 3.0845, Train Acc: 0.1546, Val Loss: 2.6024, Val Acc: 0.1923\nEpoch [2/35] Train Loss: 2.1020, Train Acc: 0.4381, Val Loss: 2.2863, Val Acc: 0.3846\nEpoch [3/35] Train Loss: 1.5394, Train Acc: 0.6572, Val Loss: 1.9127, Val Acc: 0.4615\nEpoch [4/35] Train Loss: 1.1101, Train Acc: 0.7655, Val Loss: 1.7135, Val Acc: 0.5000\nEpoch [5/35] Train Loss: 0.7350, Train Acc: 0.8840, Val Loss: 1.7070, Val Acc: 0.4615\nEpoch [6/35] Train Loss: 0.5163, Train Acc: 0.8995, Val Loss: 1.6971, Val Acc: 0.5192\nEpoch [7/35] Train Loss: 0.3278, Train Acc: 0.9536, Val Loss: 1.9010, Val Acc: 0.4038\nEpoch 00007: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [8/35] Train Loss: 0.2410, Train Acc: 0.9716, Val Loss: 1.6179, Val Acc: 0.4808\nEpoch [9/35] Train Loss: 0.2434, Train Acc: 0.9691, Val Loss: 1.5461, Val Acc: 0.5577\nEpoch [10/35] Train Loss: 0.2113, Train Acc: 0.9794, Val Loss: 1.4904, Val Acc: 0.5577\nEpoch [11/35] Train Loss: 0.1910, Train Acc: 0.9871, Val Loss: 1.5731, Val Acc: 0.5385\nEpoch [12/35] Train Loss: 0.1933, Train Acc: 0.9742, Val Loss: 1.5754, Val Acc: 0.5385\nEpoch [13/35] Train Loss: 0.1849, Train Acc: 0.9768, Val Loss: 1.6016, Val Acc: 0.5385\nEpoch 00013: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [14/35] Train Loss: 0.1845, Train Acc: 0.9897, Val Loss: 1.6757, Val Acc: 0.5385\nEpoch [15/35] Train Loss: 0.1858, Train Acc: 0.9768, Val Loss: 1.6552, Val Acc: 0.5577\nEpoch [16/35] Train Loss: 0.2049, Train Acc: 0.9742, Val Loss: 1.6888, Val Acc: 0.5000\nEpoch [17/35] Train Loss: 0.1763, Train Acc: 0.9768, Val Loss: 1.6636, Val Acc: 0.5192\nEpoch [18/35] Train Loss: 0.1816, Train Acc: 0.9897, Val Loss: 1.5845, Val Acc: 0.5577\nEpoch [19/35] Train Loss: 0.1815, Train Acc: 0.9845, Val Loss: 1.7062, Val Acc: 0.5385\nEpoch 00019: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [20/35] Train Loss: 0.1783, Train Acc: 0.9820, Val Loss: 1.6492, Val Acc: 0.5385\nEpoch [21/35] Train Loss: 0.1928, Train Acc: 0.9716, Val Loss: 1.5760, Val Acc: 0.5385\nEpoch [22/35] Train Loss: 0.1763, Train Acc: 0.9845, Val Loss: 1.6309, Val Acc: 0.5385\nEpoch [23/35] Train Loss: 0.1847, Train Acc: 0.9742, Val Loss: 1.6585, Val Acc: 0.5385\nEpoch [24/35] Train Loss: 0.1874, Train Acc: 0.9742, Val Loss: 1.6058, Val Acc: 0.5577\nEpoch [25/35] Train Loss: 0.1898, Train Acc: 0.9845, Val Loss: 1.6203, Val Acc: 0.5192\nEarly stopping triggered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.2\n",
        "num_ftrs = model_copy.fc.in_features\n",
        "model2.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(num_ftrs, 43)\n",
        "            )\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T20:07:41.205790Z",
          "iopub.execute_input": "2024-07-01T20:07:41.206166Z",
          "iopub.status.idle": "2024-07-01T20:07:41.212215Z",
          "shell.execute_reply.started": "2024-07-01T20:07:41.206137Z",
          "shell.execute_reply": "2024-07-01T20:07:41.211341Z"
        },
        "trusted": true,
        "id": "w_0Wg5zGTfOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 35\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_rest = model2.to(device)#CustomDenseNet(num_classes, growth_rate=32).to(device)\n",
        "optimizer_rest = optim.AdamW(model_rest.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler_rest = ReduceLROnPlateau(optimizer_rest, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "model_rest = train_model_customCNN(model_rest, nn.CrossEntropyLoss(), optimizer_rest, scheduler_rest, train_loader_rest, val_loader_rest, num_epochs=epochs, patience= 10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T20:11:29.443755Z",
          "iopub.execute_input": "2024-07-01T20:11:29.444635Z",
          "iopub.status.idle": "2024-07-01T20:13:19.235770Z",
          "shell.execute_reply.started": "2024-07-01T20:11:29.444600Z",
          "shell.execute_reply": "2024-07-01T20:13:19.234698Z"
        },
        "trusted": true,
        "id": "qsUbyHr0TfOM",
        "outputId": "d7fc0123-9f22-47f6-d69e-dc5cbc699780"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/35] Train Loss: 2.8738, Train Acc: 0.2026, Val Loss: 2.3944, Val Acc: 0.1124\nEpoch [2/35] Train Loss: 1.8022, Train Acc: 0.5115, Val Loss: 1.7449, Val Acc: 0.3820\nEpoch [3/35] Train Loss: 1.2416, Train Acc: 0.6987, Val Loss: 1.4890, Val Acc: 0.6067\nEpoch [4/35] Train Loss: 0.8471, Train Acc: 0.7859, Val Loss: 1.3650, Val Acc: 0.6854\nEpoch [5/35] Train Loss: 0.5957, Train Acc: 0.8487, Val Loss: 1.2808, Val Acc: 0.6629\nEpoch [6/35] Train Loss: 0.4596, Train Acc: 0.8923, Val Loss: 1.3580, Val Acc: 0.5730\nEpoch [7/35] Train Loss: 0.3549, Train Acc: 0.9128, Val Loss: 1.3606, Val Acc: 0.6180\nEpoch 00007: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [8/35] Train Loss: 0.2621, Train Acc: 0.9410, Val Loss: 1.2777, Val Acc: 0.6629\nEpoch [9/35] Train Loss: 0.2302, Train Acc: 0.9590, Val Loss: 1.2469, Val Acc: 0.6629\nEpoch [10/35] Train Loss: 0.2118, Train Acc: 0.9641, Val Loss: 1.2246, Val Acc: 0.6517\nEpoch [11/35] Train Loss: 0.2228, Train Acc: 0.9474, Val Loss: 1.2186, Val Acc: 0.6742\nEpoch [12/35] Train Loss: 0.2160, Train Acc: 0.9641, Val Loss: 1.2278, Val Acc: 0.6742\nEpoch [13/35] Train Loss: 0.1874, Train Acc: 0.9679, Val Loss: 1.2478, Val Acc: 0.6742\nEpoch 00013: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [14/35] Train Loss: 0.1849, Train Acc: 0.9551, Val Loss: 1.2520, Val Acc: 0.6629\nEpoch [15/35] Train Loss: 0.1692, Train Acc: 0.9679, Val Loss: 1.2635, Val Acc: 0.6742\nEpoch [16/35] Train Loss: 0.1971, Train Acc: 0.9590, Val Loss: 1.2541, Val Acc: 0.6629\nEpoch [17/35] Train Loss: 0.1827, Train Acc: 0.9641, Val Loss: 1.2646, Val Acc: 0.6517\nEpoch [18/35] Train Loss: 0.1906, Train Acc: 0.9526, Val Loss: 1.2673, Val Acc: 0.6629\nEpoch [19/35] Train Loss: 0.1785, Train Acc: 0.9731, Val Loss: 1.2864, Val Acc: 0.6404\nEpoch 00019: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [20/35] Train Loss: 0.1662, Train Acc: 0.9628, Val Loss: 1.2351, Val Acc: 0.6742\nEpoch [21/35] Train Loss: 0.1834, Train Acc: 0.9590, Val Loss: 1.2539, Val Acc: 0.6629\nEarly stopping triggered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.2\n",
        "num_ftrs = model_copy.fc.in_features\n",
        "model3.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(num_ftrs, 43)\n",
        "            )\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T20:15:20.095118Z",
          "iopub.execute_input": "2024-07-01T20:15:20.095827Z",
          "iopub.status.idle": "2024-07-01T20:15:20.102188Z",
          "shell.execute_reply.started": "2024-07-01T20:15:20.095793Z",
          "shell.execute_reply": "2024-07-01T20:15:20.101305Z"
        },
        "trusted": true,
        "id": "ruM_iqbqTfON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_high_dens = CustomDenseNet(num_classes)\n",
        "model_high_dens = model3.to(device)#model_high_dens.to(device)\n",
        "optimizer_high = optim.AdamW(model_high_dens.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler_high = ReduceLROnPlateau(optimizer_high, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "#model_high = train_individual_model(model_high, train_loader_high, nn.CrossEntropyLoss(), optimizer_high, scheduler_high)\n",
        "model_high_dens = train_model_customCNN(model_high_dens, nn.CrossEntropyLoss(), optimizer_high, scheduler_high, train_loader_high, val_loader_high, num_epochs=epochs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T20:15:41.291800Z",
          "iopub.execute_input": "2024-07-01T20:15:41.292174Z",
          "iopub.status.idle": "2024-07-01T20:20:41.140558Z",
          "shell.execute_reply.started": "2024-07-01T20:15:41.292149Z",
          "shell.execute_reply": "2024-07-01T20:20:41.139246Z"
        },
        "trusted": true,
        "id": "mXiV5r-oTfON",
        "outputId": "99331b70-4e4e-4475-b41b-2f4dc2db7b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/35] Train Loss: 1.8459, Train Acc: 0.4409, Val Loss: 1.0088, Val Acc: 0.6000\nEpoch [2/35] Train Loss: 0.8224, Train Acc: 0.7242, Val Loss: 0.7362, Val Acc: 0.7613\nEpoch [3/35] Train Loss: 0.4455, Train Acc: 0.8621, Val Loss: 0.5737, Val Acc: 0.7871\nEpoch [4/35] Train Loss: 0.3288, Train Acc: 0.8893, Val Loss: 0.5664, Val Acc: 0.8129\nEpoch [5/35] Train Loss: 0.2552, Train Acc: 0.9260, Val Loss: 0.5851, Val Acc: 0.7806\nEpoch [6/35] Train Loss: 0.1690, Train Acc: 0.9497, Val Loss: 0.6230, Val Acc: 0.8129\nEpoch [7/35] Train Loss: 0.1599, Train Acc: 0.9484, Val Loss: 0.7037, Val Acc: 0.7548\nEpoch 00007: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [8/35] Train Loss: 0.1180, Train Acc: 0.9613, Val Loss: 0.5771, Val Acc: 0.8065\nEpoch [9/35] Train Loss: 0.0978, Train Acc: 0.9715, Val Loss: 0.5593, Val Acc: 0.8000\nEpoch [10/35] Train Loss: 0.0928, Train Acc: 0.9749, Val Loss: 0.5702, Val Acc: 0.7935\nEpoch [11/35] Train Loss: 0.0735, Train Acc: 0.9810, Val Loss: 0.5426, Val Acc: 0.8194\nEpoch [12/35] Train Loss: 0.0807, Train Acc: 0.9769, Val Loss: 0.5566, Val Acc: 0.8065\nEpoch [13/35] Train Loss: 0.0798, Train Acc: 0.9769, Val Loss: 0.5448, Val Acc: 0.8065\nEpoch 00013: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [14/35] Train Loss: 0.0694, Train Acc: 0.9803, Val Loss: 0.5607, Val Acc: 0.8129\nEpoch [15/35] Train Loss: 0.0629, Train Acc: 0.9857, Val Loss: 0.5575, Val Acc: 0.8065\nEpoch [16/35] Train Loss: 0.0765, Train Acc: 0.9776, Val Loss: 0.5662, Val Acc: 0.8065\nEpoch [17/35] Train Loss: 0.0708, Train Acc: 0.9823, Val Loss: 0.5693, Val Acc: 0.8065\nEpoch [18/35] Train Loss: 0.0773, Train Acc: 0.9776, Val Loss: 0.5724, Val Acc: 0.7935\nEpoch [19/35] Train Loss: 0.0674, Train Acc: 0.9817, Val Loss: 0.5232, Val Acc: 0.8258\nEpoch 00019: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [20/35] Train Loss: 0.0645, Train Acc: 0.9823, Val Loss: 0.5535, Val Acc: 0.8065\nEpoch [21/35] Train Loss: 0.0702, Train Acc: 0.9789, Val Loss: 0.5479, Val Acc: 0.8129\nEpoch [22/35] Train Loss: 0.0757, Train Acc: 0.9823, Val Loss: 0.5964, Val Acc: 0.8065\nEpoch [23/35] Train Loss: 0.0615, Train Acc: 0.9810, Val Loss: 0.5359, Val Acc: 0.8194\nEpoch [24/35] Train Loss: 0.0640, Train Acc: 0.9864, Val Loss: 0.5572, Val Acc: 0.8129\nEpoch [25/35] Train Loss: 0.0726, Train Acc: 0.9817, Val Loss: 0.6055, Val Acc: 0.8065\nEpoch 00025: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [26/35] Train Loss: 0.0529, Train Acc: 0.9898, Val Loss: 0.5658, Val Acc: 0.8129\nEpoch [27/35] Train Loss: 0.0647, Train Acc: 0.9823, Val Loss: 0.5415, Val Acc: 0.8129\nEpoch [28/35] Train Loss: 0.0582, Train Acc: 0.9844, Val Loss: 0.5515, Val Acc: 0.8194\nEpoch [29/35] Train Loss: 0.0600, Train Acc: 0.9878, Val Loss: 0.5749, Val Acc: 0.8065\nEpoch [30/35] Train Loss: 0.0651, Train Acc: 0.9830, Val Loss: 0.5555, Val Acc: 0.8000\nEpoch [31/35] Train Loss: 0.0665, Train Acc: 0.9844, Val Loss: 0.5490, Val Acc: 0.8065\nEpoch 00031: reducing learning rate of group 0 to 1.0000e-08.\nEpoch [32/35] Train Loss: 0.0660, Train Acc: 0.9823, Val Loss: 0.5373, Val Acc: 0.8129\nEpoch [33/35] Train Loss: 0.0525, Train Acc: 0.9885, Val Loss: 0.5489, Val Acc: 0.8194\nEpoch [34/35] Train Loss: 0.0671, Train Acc: 0.9837, Val Loss: 0.5719, Val Acc: 0.8129\nEarly stopping triggered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleModelComplexorg(nn.Module):\n",
        "    def __init__(self, model_50, model_high, model_low, num_classes):\n",
        "        super(EnsembleModelComplex, self).__init__()\n",
        "        self.model_50 = model_50[0]\n",
        "        self.model_high = model_high[0]\n",
        "        self.model_low = model_low[0]\n",
        "\n",
        "        # Freeze the submodels' parameters\n",
        "        for param in self.model_50.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.model_high.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.model_low.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Define a more complex combination network\n",
        "        self.fc1 = nn.Linear(num_classes * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_50 = self.model_50(x)\n",
        "        out_high = self.model_high(x)\n",
        "        out_low = self.model_low(x)\n",
        "\n",
        "        out = torch.cat((out_50, out_high, out_low), dim=1)\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.fc2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def train_ensemble_model_complex(ensemble_model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=5):\n",
        "    best_val_acc = 0.0\n",
        "    best_model_weights = None\n",
        "    no_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ensemble_model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = ensemble_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        val_acc = evaluate_model_complex(ensemble_model, val_loader)\n",
        "        print(f'Validation Accuracy: {val_acc:.4f}')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_weights = ensemble_model.state_dict().copy()\n",
        "            no_improvement = 0\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "\n",
        "        if no_improvement >= patience:\n",
        "            print('Early stopping due to no improvement')\n",
        "            break\n",
        "\n",
        "    if best_model_weights:\n",
        "        ensemble_model.load_state_dict(best_model_weights)\n",
        "\n",
        "    return ensemble_model\n",
        "\n",
        "def evaluate_model_complex(ensemble_model, val_loader):\n",
        "    ensemble_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = ensemble_model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return correct / total\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T21:33:12.239152Z",
          "iopub.execute_input": "2024-07-01T21:33:12.239991Z",
          "iopub.status.idle": "2024-07-01T21:33:12.258316Z",
          "shell.execute_reply.started": "2024-07-01T21:33:12.239958Z",
          "shell.execute_reply": "2024-07-01T21:33:12.257238Z"
        },
        "trusted": true,
        "id": "Pk0nceFkTfON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the ensemble model\n",
        "ensemble_model_complex = EnsembleModelComplex(model_rest, model_high_dens, model_low_dens, num_classes).to(device)\n",
        "\n",
        "# Define loss, optimizer, and scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ensemble = optim.Adam(ensemble_model_complex.parameters(), lr=0.001)  # Optimize all parameters in the complex model\n",
        "scheduler_ensemble = optim.lr_scheduler.StepLR(optimizer_ensemble, step_size=5, gamma=0.5)\n",
        "\n",
        "# Train the ensemble model\n",
        "ensemble_model_complex = train_ensemble_model_complex(ensemble_model_complex, train_loader, val_loader, criterion, optimizer_ensemble, scheduler_ensemble, patience= 10)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T20:50:15.785012Z",
          "iopub.execute_input": "2024-07-01T20:50:15.785881Z",
          "iopub.status.idle": "2024-07-01T20:57:32.313422Z",
          "shell.execute_reply.started": "2024-07-01T20:50:15.785816Z",
          "shell.execute_reply": "2024-07-01T20:57:32.312203Z"
        },
        "trusted": true,
        "id": "8sfVoUmzTfON",
        "outputId": "26dd941f-a302-4380-c250-acd9c376522c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/25] Loss: 2.8475, Accuracy: 0.2705\nValidation Accuracy: 0.4257\nEpoch [2/25] Loss: 2.0796, Accuracy: 0.4186\nValidation Accuracy: 0.4662\nEpoch [3/25] Loss: 1.7805, Accuracy: 0.4924\nValidation Accuracy: 0.5068\nEpoch [4/25] Loss: 1.6591, Accuracy: 0.5121\nValidation Accuracy: 0.4696\nEpoch [5/25] Loss: 1.5485, Accuracy: 0.5420\nValidation Accuracy: 0.5169\nEpoch [6/25] Loss: 1.3064, Accuracy: 0.6038\nValidation Accuracy: 0.5203\nEpoch [7/25] Loss: 1.2412, Accuracy: 0.6239\nValidation Accuracy: 0.5473\nEpoch [8/25] Loss: 1.2684, Accuracy: 0.6201\nValidation Accuracy: 0.5473\nEpoch [9/25] Loss: 1.1971, Accuracy: 0.6337\nValidation Accuracy: 0.5135\nEpoch [10/25] Loss: 1.2055, Accuracy: 0.6443\nValidation Accuracy: 0.5743\nEpoch [11/25] Loss: 1.1043, Accuracy: 0.6545\nValidation Accuracy: 0.5507\nEpoch [12/25] Loss: 1.0993, Accuracy: 0.6674\nValidation Accuracy: 0.5676\nEpoch [13/25] Loss: 1.0702, Accuracy: 0.6818\nValidation Accuracy: 0.5439\nEpoch [14/25] Loss: 1.0487, Accuracy: 0.6788\nValidation Accuracy: 0.5574\nEpoch [15/25] Loss: 1.0572, Accuracy: 0.6837\nValidation Accuracy: 0.5541\nEpoch [16/25] Loss: 0.9898, Accuracy: 0.7038\nValidation Accuracy: 0.5642\nEpoch [17/25] Loss: 1.0027, Accuracy: 0.6890\nValidation Accuracy: 0.5811\nEpoch [18/25] Loss: 0.9556, Accuracy: 0.7057\nValidation Accuracy: 0.5743\nEpoch [19/25] Loss: 0.9797, Accuracy: 0.7098\nValidation Accuracy: 0.5709\nEpoch [20/25] Loss: 0.9648, Accuracy: 0.6973\nValidation Accuracy: 0.5777\nEpoch [21/25] Loss: 0.9984, Accuracy: 0.6977\nValidation Accuracy: 0.5642\nEpoch [22/25] Loss: 0.9760, Accuracy: 0.7072\nValidation Accuracy: 0.5676\nEpoch [23/25] Loss: 0.9435, Accuracy: 0.7117\nValidation Accuracy: 0.5743\nEpoch [24/25] Loss: 0.9726, Accuracy: 0.7098\nValidation Accuracy: 0.5473\nEpoch [25/25] Loss: 0.9708, Accuracy: 0.6985\nValidation Accuracy: 0.5642\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copynn(model):\n",
        "    model_copy = CustomDenseNet()  # Create a new instance of model\n",
        "    model_copy.load_state_dict(model.state_dict())\n",
        "    return model_copy\n",
        "class EnsembleModelComplex(nn.Module):\n",
        "    def __init__(self, model_50, model_high, model_low, num_classes):\n",
        "        super(EnsembleModelComplex, self).__init__()\n",
        "        self.model_50 = model_50[0]\n",
        "        self.model_high = model_high[0]\n",
        "        self.model_low = model_low[0]\n",
        "\n",
        "        for param in self.model_50.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.model_high.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.model_low.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Unfreeze the last 15 layers of each model\n",
        "        for param in list(self.model_50.parameters())[-15:]:\n",
        "            param.requires_grad = True\n",
        "        for param in list(self.model_high.parameters())[-15:]:\n",
        "            param.requires_grad = True\n",
        "        for param in list(self.model_low.parameters())[-15:]:\n",
        "            param.requires_grad = True\n",
        "\n",
        "\n",
        "        # Define a more complex combination network\n",
        "        self.fc1 = nn.Linear(num_classes * 3, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_50 = self.model_50(x)\n",
        "        out_high = self.model_high(x)\n",
        "        out_low = self.model_low(x)\n",
        "\n",
        "        out = torch.cat((out_50, out_high, out_low), dim=1)\n",
        "        out = self.relu(self.fc1(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.relu(self.fc2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc3(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "def train_ensemble_model_complex(ensemble_model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25, patience=5):\n",
        "    best_val_acc = 0.0\n",
        "    best_model_weights = None\n",
        "    no_improvement = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        ensemble_model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = ensemble_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / total\n",
        "        epoch_acc = correct / total\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        val_acc = evaluate_model_complex(ensemble_model, val_loader)\n",
        "        print(f'Validation Accuracy: {val_acc:.4f}')\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_model_weights = ensemble_model.state_dict().copy()\n",
        "            no_improvement = 0\n",
        "        else:\n",
        "            no_improvement += 1\n",
        "\n",
        "        if no_improvement >= patience:\n",
        "            print('Early stopping due to no improvement')\n",
        "            break\n",
        "\n",
        "    if best_model_weights:\n",
        "        ensemble_model.load_state_dict(best_model_weights)\n",
        "\n",
        "    return ensemble_model\n",
        "\n",
        "def evaluate_model_complex(ensemble_model, val_loader):\n",
        "    ensemble_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = ensemble_model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T21:25:30.260587Z",
          "iopub.execute_input": "2024-07-01T21:25:30.261424Z",
          "iopub.status.idle": "2024-07-01T21:25:30.282336Z",
          "shell.execute_reply.started": "2024-07-01T21:25:30.261389Z",
          "shell.execute_reply": "2024-07-01T21:25:30.281367Z"
        },
        "trusted": true,
        "id": "a3mHGw4mTfON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "model_copy1 = copy.deepcopy(model_rest)\n",
        "model_copy2 = copy.deepcopy(model_high_dens)\n",
        "model_copy3 = copy.deepcopy(model_low_dens)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T21:24:51.186071Z",
          "iopub.execute_input": "2024-07-01T21:24:51.186935Z",
          "iopub.status.idle": "2024-07-01T21:24:51.361310Z",
          "shell.execute_reply.started": "2024-07-01T21:24:51.186900Z",
          "shell.execute_reply": "2024-07-01T21:24:51.360480Z"
        },
        "trusted": true,
        "id": "HXvrGNiaTfON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the ensemble model\n",
        "ensemble_model_complex = EnsembleModelComplex(model_copy1, model_copy2, model_copy3, num_classes).to(device)\n",
        "\n",
        "# Define loss, optimizer, and scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ensemble = optim.Adam(ensemble_model_complex.parameters(), lr=0.001)  # Optimize all parameters in the complex model\n",
        "scheduler_ensemble = optim.lr_scheduler.StepLR(optimizer_ensemble, step_size=5, gamma=0.5)\n",
        "\n",
        "# Train the ensemble model\n",
        "ensemble_model_complex = train_ensemble_model_complex(ensemble_model_complex, train_loader_50, val_loader_50, criterion, optimizer_ensemble, scheduler_ensemble, patience= 10)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T21:25:32.519876Z",
          "iopub.execute_input": "2024-07-01T21:25:32.520226Z",
          "iopub.status.idle": "2024-07-01T21:31:36.735372Z",
          "shell.execute_reply.started": "2024-07-01T21:25:32.520198Z",
          "shell.execute_reply": "2024-07-01T21:31:36.734268Z"
        },
        "trusted": true,
        "id": "GCtvUu5-TfOO",
        "outputId": "37a4e3c3-1ae2-43b1-db2c-64c0d48e7a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/25] Loss: 3.6366, Accuracy: 0.0655\nValidation Accuracy: 0.2264\nEpoch [2/25] Loss: 2.7190, Accuracy: 0.2174\nValidation Accuracy: 0.3784\nEpoch [3/25] Loss: 1.8909, Accuracy: 0.4073\nValidation Accuracy: 0.5270\nEpoch [4/25] Loss: 1.3553, Accuracy: 0.5763\nValidation Accuracy: 0.4764\nEpoch [5/25] Loss: 1.0685, Accuracy: 0.6582\nValidation Accuracy: 0.4696\nEpoch [6/25] Loss: 0.8863, Accuracy: 0.7118\nValidation Accuracy: 0.5236\nEpoch [7/25] Loss: 0.7184, Accuracy: 0.7595\nValidation Accuracy: 0.5236\nEpoch [8/25] Loss: 0.6394, Accuracy: 0.7908\nValidation Accuracy: 0.5236\nEpoch [9/25] Loss: 0.5658, Accuracy: 0.8302\nValidation Accuracy: 0.4865\nEpoch [10/25] Loss: 0.5094, Accuracy: 0.8399\nValidation Accuracy: 0.5270\nEpoch [11/25] Loss: 0.4855, Accuracy: 0.8459\nValidation Accuracy: 0.5338\nEpoch [12/25] Loss: 0.4280, Accuracy: 0.8615\nValidation Accuracy: 0.5439\nEpoch [13/25] Loss: 0.4021, Accuracy: 0.8794\nValidation Accuracy: 0.5203\nEpoch [14/25] Loss: 0.4202, Accuracy: 0.8697\nValidation Accuracy: 0.5372\nEpoch [15/25] Loss: 0.4320, Accuracy: 0.8749\nValidation Accuracy: 0.4966\nEpoch [16/25] Loss: 0.3108, Accuracy: 0.9010\nValidation Accuracy: 0.5101\nEpoch [17/25] Loss: 0.3311, Accuracy: 0.9047\nValidation Accuracy: 0.5338\nEpoch [18/25] Loss: 0.3561, Accuracy: 0.8965\nValidation Accuracy: 0.5473\nEpoch [19/25] Loss: 0.3005, Accuracy: 0.9047\nValidation Accuracy: 0.5270\nEpoch [20/25] Loss: 0.3089, Accuracy: 0.9032\nValidation Accuracy: 0.5405\nEpoch [21/25] Loss: 0.3026, Accuracy: 0.9106\nValidation Accuracy: 0.5304\nEpoch [22/25] Loss: 0.2902, Accuracy: 0.9129\nValidation Accuracy: 0.5338\nEpoch [23/25] Loss: 0.2924, Accuracy: 0.9062\nValidation Accuracy: 0.5135\nEpoch [24/25] Loss: 0.2947, Accuracy: 0.9084\nValidation Accuracy: 0.5203\nEpoch [25/25] Loss: 0.2581, Accuracy: 0.9226\nValidation Accuracy: 0.5405\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the ensemble model\n",
        "ensemble_model_complex = EnsembleModelComplexorg(model_copy1, model_copy2, model_copy3, num_classes).to(device)\n",
        "\n",
        "# Define loss, optimizer, and scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ensemble = optim.Adam(ensemble_model_complex.parameters(), lr=0.001)  # Optimize all parameters in the complex model\n",
        "scheduler_ensemble = optim.lr_scheduler.StepLR(optimizer_ensemble, step_size=5, gamma=0.5)\n",
        "\n",
        "# Train the ensemble model\n",
        "ensemble_model_complex = train_ensemble_model_complex(ensemble_model_complex, train_loader, val_loader, criterion, optimizer_ensemble, scheduler_ensemble, patience= 10)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-01T21:33:23.616524Z",
          "iopub.execute_input": "2024-07-01T21:33:23.617175Z",
          "iopub.status.idle": "2024-07-01T21:33:23.672062Z",
          "shell.execute_reply.started": "2024-07-01T21:33:23.617140Z",
          "shell.execute_reply": "2024-07-01T21:33:23.670910Z"
        },
        "trusted": true,
        "id": "gy-wutRhTfOO",
        "outputId": "88b18994-e23d-4f56-c845-3c031477c139"
      },
      "execution_count": null,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[77], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Instantiate the ensemble model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ensemble_model_complex \u001b[38;5;241m=\u001b[39m \u001b[43mEnsembleModelComplexorg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_copy1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_copy2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_copy3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define loss, optimizer, and scheduler\u001b[39;00m\n\u001b[1;32m      5\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
            "Cell \u001b[0;32mIn[76], line 3\u001b[0m, in \u001b[0;36mEnsembleModelComplexorg.__init__\u001b[0;34m(self, model_50, model_high, model_low, num_classes)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_50, model_high, model_low, num_classes):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mEnsembleModelComplex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_50 \u001b[38;5;241m=\u001b[39m model_50[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_high \u001b[38;5;241m=\u001b[39m model_high[\u001b[38;5;241m0\u001b[39m]\n",
            "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
          ],
          "ename": "TypeError",
          "evalue": "super(type, obj): obj must be an instance or subtype of type",
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S7ugRM7VTfOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 35\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_low_dens = model#CustomDenseNet(num_classes)\n",
        "model_low_dens = model_low_dens.to(device)\n",
        "optimizer_low = optim.AdamW(model_low_dens.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler_low = ReduceLROnPlateau(optimizer_low, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "#model_high = train_individual_model(model_high, train_loader_high, nn.CrossEntropyLoss(), optimizer_high, scheduler_high)\n",
        "model_low_dens = train_model_customCNN(model_low_dens, nn.CrossEntropyLoss(), optimizer_low, scheduler_low, train_loader_low, val_loader_low, num_epochs=epochs)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:38:46.418745Z",
          "iopub.execute_input": "2024-07-02T17:38:46.419110Z",
          "iopub.status.idle": "2024-07-02T17:39:30.885538Z",
          "shell.execute_reply.started": "2024-07-02T17:38:46.419081Z",
          "shell.execute_reply": "2024-07-02T17:39:30.884440Z"
        },
        "trusted": true,
        "id": "WiHA2ULkTfOS",
        "outputId": "784f5972-9f32-4c80-93cf-34293056c1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/35] Train Loss: 2.7519, Train Acc: 0.4046, Val Loss: 2.2759, Val Acc: 0.3654\nEpoch [2/35] Train Loss: 1.3356, Train Acc: 0.8840, Val Loss: 1.8725, Val Acc: 0.5000\nEpoch [3/35] Train Loss: 0.7086, Train Acc: 0.9407, Val Loss: 1.6482, Val Acc: 0.5192\nEpoch [4/35] Train Loss: 0.4226, Train Acc: 0.9820, Val Loss: 1.6437, Val Acc: 0.5385\nEpoch [5/35] Train Loss: 0.2764, Train Acc: 0.9768, Val Loss: 1.6792, Val Acc: 0.5385\nEpoch [6/35] Train Loss: 0.2120, Train Acc: 0.9794, Val Loss: 1.6762, Val Acc: 0.5192\nEpoch [7/35] Train Loss: 0.1739, Train Acc: 0.9845, Val Loss: 1.6851, Val Acc: 0.5577\nEpoch 00007: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [8/35] Train Loss: 0.1564, Train Acc: 0.9871, Val Loss: 1.6955, Val Acc: 0.5577\nEpoch [9/35] Train Loss: 0.1336, Train Acc: 1.0000, Val Loss: 1.7365, Val Acc: 0.5577\nEpoch [10/35] Train Loss: 0.1478, Train Acc: 0.9820, Val Loss: 1.7070, Val Acc: 0.5577\nEpoch [11/35] Train Loss: 0.1501, Train Acc: 0.9871, Val Loss: 1.7384, Val Acc: 0.5577\nEpoch [12/35] Train Loss: 0.1446, Train Acc: 0.9845, Val Loss: 1.7290, Val Acc: 0.5577\nEpoch [13/35] Train Loss: 0.1435, Train Acc: 0.9845, Val Loss: 1.7073, Val Acc: 0.5385\nEpoch 00013: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [14/35] Train Loss: 0.1365, Train Acc: 0.9897, Val Loss: 1.7372, Val Acc: 0.5577\nEpoch [15/35] Train Loss: 0.1184, Train Acc: 0.9974, Val Loss: 1.6870, Val Acc: 0.5577\nEpoch [16/35] Train Loss: 0.1516, Train Acc: 0.9794, Val Loss: 1.7129, Val Acc: 0.5577\nEpoch [17/35] Train Loss: 0.1415, Train Acc: 0.9871, Val Loss: 1.7337, Val Acc: 0.5769\nEpoch [18/35] Train Loss: 0.1210, Train Acc: 0.9871, Val Loss: 1.7599, Val Acc: 0.5577\nEpoch [19/35] Train Loss: 0.1605, Train Acc: 0.9871, Val Loss: 1.6795, Val Acc: 0.5577\nEarly stopping triggered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.2\n",
        "num_ftrs = model_copy.fc.in_features\n",
        "model2.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(num_ftrs, 43)\n",
        "            )\n",
        "epochs = 35\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_rest = model2.to(device)#CustomDenseNet(num_classes, growth_rate=32).to(device)\n",
        "optimizer_rest = optim.AdamW(model_rest.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler_rest = ReduceLROnPlateau(optimizer_rest, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "model_rest = train_model_customCNN(model_rest, nn.CrossEntropyLoss(), optimizer_rest, scheduler_rest, train_loader_rest, val_loader_rest, num_epochs=epochs, patience= 10)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:39:48.271322Z",
          "iopub.execute_input": "2024-07-02T17:39:48.271711Z",
          "iopub.status.idle": "2024-07-02T17:42:03.768798Z",
          "shell.execute_reply.started": "2024-07-02T17:39:48.271677Z",
          "shell.execute_reply": "2024-07-02T17:42:03.767607Z"
        },
        "trusted": true,
        "id": "-qwkE3MHTfOT",
        "outputId": "56dec8e0-ef14-4969-9aa7-bd65cbc610f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/35] Train Loss: 2.8505, Train Acc: 0.2321, Val Loss: 2.3561, Val Acc: 0.1124\nEpoch [2/35] Train Loss: 1.8153, Train Acc: 0.4859, Val Loss: 1.8799, Val Acc: 0.3933\nEpoch [3/35] Train Loss: 1.2275, Train Acc: 0.6910, Val Loss: 1.6310, Val Acc: 0.5393\nEpoch [4/35] Train Loss: 0.8685, Train Acc: 0.7718, Val Loss: 1.3124, Val Acc: 0.5730\nEpoch [5/35] Train Loss: 0.6270, Train Acc: 0.8333, Val Loss: 1.3152, Val Acc: 0.5955\nEpoch [6/35] Train Loss: 0.4629, Train Acc: 0.8821, Val Loss: 1.2108, Val Acc: 0.6854\nEpoch [7/35] Train Loss: 0.3576, Train Acc: 0.9115, Val Loss: 1.2824, Val Acc: 0.6180\nEpoch 00007: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [8/35] Train Loss: 0.2701, Train Acc: 0.9282, Val Loss: 1.2309, Val Acc: 0.6292\nEpoch [9/35] Train Loss: 0.2609, Train Acc: 0.9436, Val Loss: 1.1780, Val Acc: 0.6629\nEpoch [10/35] Train Loss: 0.2500, Train Acc: 0.9449, Val Loss: 1.1999, Val Acc: 0.6966\nEpoch [11/35] Train Loss: 0.2083, Train Acc: 0.9551, Val Loss: 1.2407, Val Acc: 0.6854\nEpoch [12/35] Train Loss: 0.1939, Train Acc: 0.9590, Val Loss: 1.2160, Val Acc: 0.6742\nEpoch [13/35] Train Loss: 0.1835, Train Acc: 0.9654, Val Loss: 1.1970, Val Acc: 0.6742\nEpoch 00013: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [14/35] Train Loss: 0.1820, Train Acc: 0.9603, Val Loss: 1.1897, Val Acc: 0.6854\nEpoch [15/35] Train Loss: 0.1829, Train Acc: 0.9577, Val Loss: 1.1819, Val Acc: 0.6742\nEpoch [16/35] Train Loss: 0.1711, Train Acc: 0.9744, Val Loss: 1.1594, Val Acc: 0.7079\nEpoch [17/35] Train Loss: 0.1697, Train Acc: 0.9731, Val Loss: 1.2055, Val Acc: 0.6854\nEpoch [18/35] Train Loss: 0.1776, Train Acc: 0.9628, Val Loss: 1.1790, Val Acc: 0.6854\nEpoch [19/35] Train Loss: 0.1933, Train Acc: 0.9564, Val Loss: 1.2070, Val Acc: 0.6854\nEpoch 00019: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [20/35] Train Loss: 0.1795, Train Acc: 0.9667, Val Loss: 1.1949, Val Acc: 0.6742\nEpoch [21/35] Train Loss: 0.1947, Train Acc: 0.9487, Val Loss: 1.2042, Val Acc: 0.6629\nEpoch [22/35] Train Loss: 0.1930, Train Acc: 0.9577, Val Loss: 1.2128, Val Acc: 0.6854\nEpoch [23/35] Train Loss: 0.1999, Train Acc: 0.9538, Val Loss: 1.2069, Val Acc: 0.6854\nEpoch [24/35] Train Loss: 0.1784, Train Acc: 0.9679, Val Loss: 1.2109, Val Acc: 0.6742\nEpoch [25/35] Train Loss: 0.1866, Train Acc: 0.9628, Val Loss: 1.1692, Val Acc: 0.6966\nEpoch 00025: reducing learning rate of group 0 to 1.0000e-07.\nEpoch [26/35] Train Loss: 0.2052, Train Acc: 0.9564, Val Loss: 1.1661, Val Acc: 0.7191\nEarly stopping triggered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_rate = 0.2\n",
        "num_ftrs = model_copy.fc.in_features\n",
        "model3.fc = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(num_ftrs, 43)\n",
        "            )\n",
        "#model_high_dens = CustomDenseNet(num_classes)\n",
        "model_high_dens = model3.to(device)#model_high_dens.to(device)\n",
        "optimizer_high = optim.AdamW(model_high_dens.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler_high = ReduceLROnPlateau(optimizer_high, mode='max', factor=0.1, patience=5, verbose=True)\n",
        "#model_high = train_individual_model(model_high, train_loader_high, nn.CrossEntropyLoss(), optimizer_high, scheduler_high)\n",
        "model_high_dens = train_model_customCNN(model_high_dens, nn.CrossEntropyLoss(), optimizer_high, scheduler_high, train_loader_high, val_loader_high, num_epochs=epochs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:42:03.770774Z",
          "iopub.execute_input": "2024-07-02T17:42:03.771111Z",
          "iopub.status.idle": "2024-07-02T17:45:36.144340Z",
          "shell.execute_reply.started": "2024-07-02T17:42:03.771076Z",
          "shell.execute_reply": "2024-07-02T17:45:36.143183Z"
        },
        "trusted": true,
        "id": "bzl9qE4ETfOT",
        "outputId": "cf0dcc5a-b6a4-40c2-9c9c-616ec9617fca"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/35] Train Loss: 1.8865, Train Acc: 0.4130, Val Loss: 1.0590, Val Acc: 0.6065\nEpoch [2/35] Train Loss: 0.7999, Train Acc: 0.7391, Val Loss: 0.6596, Val Acc: 0.7548\nEpoch [3/35] Train Loss: 0.4710, Train Acc: 0.8471, Val Loss: 0.6752, Val Acc: 0.7742\nEpoch [4/35] Train Loss: 0.3643, Train Acc: 0.8838, Val Loss: 0.5360, Val Acc: 0.8129\nEpoch [5/35] Train Loss: 0.2404, Train Acc: 0.9198, Val Loss: 0.6884, Val Acc: 0.7548\nEpoch [6/35] Train Loss: 0.2076, Train Acc: 0.9307, Val Loss: 0.8718, Val Acc: 0.7484\nEpoch [7/35] Train Loss: 0.1492, Train Acc: 0.9524, Val Loss: 0.5193, Val Acc: 0.8194\nEpoch 00007: reducing learning rate of group 0 to 1.0000e-04.\nEpoch [8/35] Train Loss: 0.1445, Train Acc: 0.9572, Val Loss: 0.4530, Val Acc: 0.8194\nEpoch [9/35] Train Loss: 0.1084, Train Acc: 0.9681, Val Loss: 0.4404, Val Acc: 0.8258\nEpoch [10/35] Train Loss: 0.0917, Train Acc: 0.9789, Val Loss: 0.4674, Val Acc: 0.8194\nEpoch [11/35] Train Loss: 0.0899, Train Acc: 0.9715, Val Loss: 0.4965, Val Acc: 0.8323\nEpoch [12/35] Train Loss: 0.0833, Train Acc: 0.9823, Val Loss: 0.4857, Val Acc: 0.8129\nEpoch [13/35] Train Loss: 0.0720, Train Acc: 0.9823, Val Loss: 0.5150, Val Acc: 0.8323\nEpoch 00013: reducing learning rate of group 0 to 1.0000e-05.\nEpoch [14/35] Train Loss: 0.0800, Train Acc: 0.9783, Val Loss: 0.4749, Val Acc: 0.8581\nEpoch [15/35] Train Loss: 0.0781, Train Acc: 0.9837, Val Loss: 0.4905, Val Acc: 0.8516\nEpoch [16/35] Train Loss: 0.0704, Train Acc: 0.9823, Val Loss: 0.5153, Val Acc: 0.8323\nEpoch [17/35] Train Loss: 0.0650, Train Acc: 0.9871, Val Loss: 0.4585, Val Acc: 0.8323\nEpoch [18/35] Train Loss: 0.0691, Train Acc: 0.9851, Val Loss: 0.4668, Val Acc: 0.8581\nEpoch [19/35] Train Loss: 0.0725, Train Acc: 0.9796, Val Loss: 0.4807, Val Acc: 0.8516\nEpoch 00019: reducing learning rate of group 0 to 1.0000e-06.\nEpoch [20/35] Train Loss: 0.0689, Train Acc: 0.9857, Val Loss: 0.4708, Val Acc: 0.8516\nEpoch [21/35] Train Loss: 0.0718, Train Acc: 0.9837, Val Loss: 0.4936, Val Acc: 0.8194\nEpoch [22/35] Train Loss: 0.0667, Train Acc: 0.9830, Val Loss: 0.4834, Val Acc: 0.8581\nEpoch [23/35] Train Loss: 0.0583, Train Acc: 0.9857, Val Loss: 0.4864, Val Acc: 0.8516\nEpoch [24/35] Train Loss: 0.0711, Train Acc: 0.9817, Val Loss: 0.4935, Val Acc: 0.8516\nEarly stopping triggered\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the ensemble model\n",
        "ensemble_model_complex = EnsembleModelComplex(model_rest, model_high_dens, model_low_dens, num_classes).to(device)\n",
        "\n",
        "# Define loss, optimizer, and scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ensemble = optim.Adam(ensemble_model_complex.parameters(), lr=0.001)  # Optimize all parameters in the complex model\n",
        "scheduler_ensemble = optim.lr_scheduler.StepLR(optimizer_ensemble, step_size=5, gamma=0.5)\n",
        "\n",
        "# Train the ensemble model\n",
        "ensemble_model_complex = train_ensemble_model_complex(ensemble_model_complex, train_loader, val_loader, criterion, optimizer_ensemble, scheduler_ensemble, patience= 10)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-02T17:45:36.146299Z",
          "iopub.execute_input": "2024-07-02T17:45:36.146627Z",
          "iopub.status.idle": "2024-07-02T17:56:04.134719Z",
          "shell.execute_reply.started": "2024-07-02T17:45:36.146592Z",
          "shell.execute_reply": "2024-07-02T17:56:04.133605Z"
        },
        "trusted": true,
        "id": "_scnDbIjTfOT",
        "outputId": "8025fa65-7122-4c4b-d31c-5f00eb6491e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch [1/25] Loss: 2.6273, Accuracy: 0.3398\nValidation Accuracy: 0.4324\nEpoch [2/25] Loss: 1.5975, Accuracy: 0.5545\nValidation Accuracy: 0.4865\nEpoch [3/25] Loss: 1.1138, Accuracy: 0.6663\nValidation Accuracy: 0.5405\nEpoch [4/25] Loss: 0.9337, Accuracy: 0.7318\nValidation Accuracy: 0.5473\nEpoch [5/25] Loss: 0.8208, Accuracy: 0.7705\nValidation Accuracy: 0.5034\nEpoch [6/25] Loss: 0.5930, Accuracy: 0.8178\nValidation Accuracy: 0.5405\nEpoch [7/25] Loss: 0.5146, Accuracy: 0.8515\nValidation Accuracy: 0.5845\nEpoch [8/25] Loss: 0.4305, Accuracy: 0.8712\nValidation Accuracy: 0.5676\nEpoch [9/25] Loss: 0.4347, Accuracy: 0.8716\nValidation Accuracy: 0.5811\nEpoch [10/25] Loss: 0.3956, Accuracy: 0.8837\nValidation Accuracy: 0.5946\nEpoch [11/25] Loss: 0.3512, Accuracy: 0.8947\nValidation Accuracy: 0.5743\nEpoch [12/25] Loss: 0.3113, Accuracy: 0.9045\nValidation Accuracy: 0.6081\nEpoch [13/25] Loss: 0.3021, Accuracy: 0.9000\nValidation Accuracy: 0.5743\nEpoch [14/25] Loss: 0.3266, Accuracy: 0.8992\nValidation Accuracy: 0.5878\nEpoch [15/25] Loss: 0.2789, Accuracy: 0.9155\nValidation Accuracy: 0.6081\nEpoch [16/25] Loss: 0.2575, Accuracy: 0.9231\nValidation Accuracy: 0.6014\nEpoch [17/25] Loss: 0.2439, Accuracy: 0.9284\nValidation Accuracy: 0.6047\nEpoch [18/25] Loss: 0.2509, Accuracy: 0.9197\nValidation Accuracy: 0.5743\nEpoch [19/25] Loss: 0.2374, Accuracy: 0.9284\nValidation Accuracy: 0.5777\nEpoch [20/25] Loss: 0.2169, Accuracy: 0.9303\nValidation Accuracy: 0.5878\nEpoch [21/25] Loss: 0.2191, Accuracy: 0.9277\nValidation Accuracy: 0.5878\nEpoch [22/25] Loss: 0.2163, Accuracy: 0.9295\nValidation Accuracy: 0.6047\nEarly stopping due to no improvement\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}